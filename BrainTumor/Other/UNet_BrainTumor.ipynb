{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b0c03-8059-4397-89ad-42e738dccf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaber\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaber\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/348 ━━━━━━━━━━━━━━━━━━━━ 23:32:30\n",
      "Accuracy: 0.5634 - Precision: 0.0542 - Recall: 0.9990 - Specificity: 0.5522 - F1: 0.1029 - Loss: 2.1023\n",
      "\n",
      "Batch 2/348 ━━━━━━━━━━━━━━━━━━━━ 23:34:45\n",
      "Accuracy: 0.6661 - Precision: 0.0409 - Recall: 0.8491 - Specificity: 0.6608 - F1: 0.0779 - Loss: 1.8676\n",
      "\n",
      "Batch 3/348 ━━━━━━━━━━━━━━━━━━━━ 23:36:48\n",
      "Accuracy: 0.7607 - Precision: 0.1198 - Recall: 0.8781 - Specificity: 0.7572 - F1: 0.1947 - Loss: 1.6727\n",
      "\n",
      "Batch 4/348 ━━━━━━━━━━━━━━━━━━━━ 23:38:44\n",
      "Accuracy: 0.8102 - Precision: 0.1193 - Recall: 0.9056 - Specificity: 0.8076 - F1: 0.1986 - Loss: 1.5667\n",
      "\n",
      "Batch 5/348 ━━━━━━━━━━━━━━━━━━━━ 23:40:36\n",
      "Accuracy: 0.8478 - Precision: 0.2699 - Recall: 0.8845 - Specificity: 0.8460 - F1: 0.3258 - Loss: 1.4882\n",
      "\n",
      "Batch 6/348 ━━━━━━━━━━━━━━━━━━━━ 23:42:30\n",
      "Accuracy: 0.8715 - Precision: 0.2444 - Recall: 0.8894 - Specificity: 0.8700 - F1: 0.3061 - Loss: 1.4362\n",
      "\n",
      "Batch 7/348 ━━━━━━━━━━━━━━━━━━━━ 23:44:34\n",
      "Accuracy: 0.8893 - Precision: 0.3498 - Recall: 0.8557 - Specificity: 0.8885 - F1: 0.3745 - Loss: 1.3850\n",
      "\n",
      "Batch 8/348 ━━━━━━━━━━━━━━━━━━━━ 23:46:30\n",
      "Accuracy: 0.9018 - Precision: 0.3612 - Recall: 0.8468 - Specificity: 0.9014 - F1: 0.3983 - Loss: 1.3487\n",
      "\n",
      "Batch 9/348 ━━━━━━━━━━━━━━━━━━━━ 23:48:19\n",
      "Accuracy: 0.9119 - Precision: 0.3585 - Recall: 0.8404 - Specificity: 0.9116 - F1: 0.4065 - Loss: 1.3232\n",
      "\n",
      "Batch 10/348 ━━━━━━━━━━━━━━━━━━━━ 23:50:04\n",
      "Accuracy: 0.9201 - Precision: 0.3754 - Recall: 0.8553 - Specificity: 0.9198 - F1: 0.4346 - Loss: 1.2958\n",
      "\n",
      "Batch 11/348 ━━━━━━━━━━━━━━━━━━━━ 23:51:52\n",
      "Accuracy: 0.9266 - Precision: 0.3534 - Recall: 0.8320 - Specificity: 0.9264 - F1: 0.4149 - Loss: 1.2803\n",
      "\n",
      "Batch 12/348 ━━━━━━━━━━━━━━━━━━━━ 23:53:20\n",
      "Accuracy: 0.9326 - Precision: 0.3656 - Recall: 0.7874 - Specificity: 0.9325 - F1: 0.4114 - Loss: 1.2657\n",
      "\n",
      "Batch 13/348 ━━━━━━━━━━━━━━━━━━━━ 23:54:56\n",
      "Accuracy: 0.9374 - Precision: 0.4058 - Recall: 0.7807 - Specificity: 0.9376 - F1: 0.4400 - Loss: 1.2439\n",
      "\n",
      "Batch 14/348 ━━━━━━━━━━━━━━━━━━━━ 23:56:34\n",
      "Accuracy: 0.9401 - Precision: 0.3824 - Recall: 0.7878 - Specificity: 0.9403 - F1: 0.4188 - Loss: 1.2337\n",
      "\n",
      "Batch 15/348 ━━━━━━━━━━━━━━━━━━━━ 23:58:04\n",
      "Accuracy: 0.9437 - Precision: 0.4094 - Recall: 0.7887 - Specificity: 0.9441 - F1: 0.4439 - Loss: 1.2102\n",
      "\n",
      "Batch 16/348 ━━━━━━━━━━━━━━━━━━━━ 23:59:43\n",
      "Accuracy: 0.9468 - Precision: 0.4181 - Recall: 0.8018 - Specificity: 0.9472 - F1: 0.4604 - Loss: 1.1953\n",
      "\n",
      "Batch 17/348 ━━━━━━━━━━━━━━━━━━━━ 00:01:29\n",
      "Accuracy: 0.9491 - Precision: 0.4164 - Recall: 0.8093 - Specificity: 0.9495 - F1: 0.4656 - Loss: 1.1800\n",
      "\n",
      "Batch 18/348 ━━━━━━━━━━━━━━━━━━━━ 00:03:06\n",
      "Accuracy: 0.9513 - Precision: 0.4026 - Recall: 0.8095 - Specificity: 0.9517 - F1: 0.4551 - Loss: 1.1728\n",
      "\n",
      "Batch 19/348 ━━━━━━━━━━━━━━━━━━━━ 00:04:40\n",
      "Accuracy: 0.9536 - Precision: 0.4277 - Recall: 0.8097 - Specificity: 0.9541 - F1: 0.4757 - Loss: 1.1566\n",
      "\n",
      "Batch 20/348 ━━━━━━━━━━━━━━━━━━━━ 00:06:12\n",
      "Accuracy: 0.9557 - Precision: 0.4224 - Recall: 0.8189 - Specificity: 0.9562 - F1: 0.4761 - Loss: 1.1496\n",
      "\n",
      "Batch 21/348 ━━━━━━━━━━━━━━━━━━━━ 00:07:45\n",
      "Accuracy: 0.9575 - Precision: 0.4123 - Recall: 0.8190 - Specificity: 0.9579 - F1: 0.4694 - Loss: 1.1442\n",
      "\n",
      "Batch 22/348 ━━━━━━━━━━━━━━━━━━━━ 00:09:22\n",
      "Accuracy: 0.9591 - Precision: 0.4335 - Recall: 0.8135 - Specificity: 0.9598 - F1: 0.4834 - Loss: 1.1283\n",
      "\n",
      "Batch 23/348 ━━━━━━━━━━━━━━━━━━━━ 00:10:57\n",
      "Accuracy: 0.9606 - Precision: 0.4233 - Recall: 0.8035 - Specificity: 0.9613 - F1: 0.4754 - Loss: 1.1246\n",
      "\n",
      "Batch 24/348 ━━━━━━━━━━━━━━━━━━━━ 00:12:36\n",
      "Accuracy: 0.9617 - Precision: 0.4206 - Recall: 0.8069 - Specificity: 0.9624 - F1: 0.4768 - Loss: 1.1163\n",
      "\n",
      "Batch 25/348 ━━━━━━━━━━━━━━━━━━━━ 00:14:03\n",
      "Accuracy: 0.9630 - Precision: 0.4341 - Recall: 0.8133 - Specificity: 0.9637 - F1: 0.4917 - Loss: 1.1000\n",
      "\n",
      "Batch 26/348 ━━━━━━━━━━━━━━━━━━━━ 00:15:43\n",
      "Accuracy: 0.9643 - Precision: 0.4504 - Recall: 0.8183 - Specificity: 0.9650 - F1: 0.5073 - Loss: 1.0848\n",
      "\n",
      "Batch 27/348 ━━━━━━━━━━━━━━━━━━━━ 00:17:27\n",
      "Accuracy: 0.9654 - Precision: 0.4506 - Recall: 0.8242 - Specificity: 0.9661 - F1: 0.5116 - Loss: 1.0793\n",
      "\n",
      "Batch 28/348 ━━━━━━━━━━━━━━━━━━━━ 00:19:03\n",
      "Accuracy: 0.9665 - Precision: 0.4642 - Recall: 0.8298 - Specificity: 0.9672 - F1: 0.5254 - Loss: 1.0683\n",
      "\n",
      "Batch 29/348 ━━━━━━━━━━━━━━━━━━━━ 00:20:45\n",
      "Accuracy: 0.9674 - Precision: 0.4655 - Recall: 0.8263 - Specificity: 0.9682 - F1: 0.5278 - Loss: 1.0625\n",
      "\n",
      "Batch 30/348 ━━━━━━━━━━━━━━━━━━━━ 00:22:32\n",
      "Accuracy: 0.9684 - Precision: 0.4797 - Recall: 0.8295 - Specificity: 0.9691 - F1: 0.5405 - Loss: 1.0464\n",
      "\n",
      "Batch 31/348 ━━━━━━━━━━━━━━━━━━━━ 00:24:05\n",
      "Accuracy: 0.9692 - Precision: 0.4868 - Recall: 0.8317 - Specificity: 0.9700 - F1: 0.5484 - Loss: 1.0368\n",
      "\n",
      "Batch 32/348 ━━━━━━━━━━━━━━━━━━━━ 00:25:55\n",
      "Accuracy: 0.9697 - Precision: 0.4835 - Recall: 0.8333 - Specificity: 0.9705 - F1: 0.5479 - Loss: 1.0312\n",
      "\n",
      "Batch 33/348 ━━━━━━━━━━━━━━━━━━━━ 00:27:40\n",
      "Accuracy: 0.9705 - Precision: 0.4930 - Recall: 0.8370 - Specificity: 0.9713 - F1: 0.5576 - Loss: 1.0200\n",
      "\n",
      "Batch 34/348 ━━━━━━━━━━━━━━━━━━━━ 00:29:17\n",
      "Accuracy: 0.9713 - Precision: 0.5063 - Recall: 0.8396 - Specificity: 0.9721 - F1: 0.5687 - Loss: 1.0067\n",
      "\n",
      "Batch 35/348 ━━━━━━━━━━━━━━━━━━━━ 00:30:57\n",
      "Accuracy: 0.9720 - Precision: 0.5105 - Recall: 0.8406 - Specificity: 0.9728 - F1: 0.5739 - Loss: 1.0008\n",
      "\n",
      "Batch 36/348 ━━━━━━━━━━━━━━━━━━━━ 00:32:35\n",
      "Accuracy: 0.9726 - Precision: 0.5128 - Recall: 0.8435 - Specificity: 0.9734 - F1: 0.5782 - Loss: 0.9945\n",
      "\n",
      "Batch 37/348 ━━━━━━━━━━━━━━━━━━━━ 00:34:12\n",
      "Accuracy: 0.9732 - Precision: 0.5204 - Recall: 0.8474 - Specificity: 0.9740 - F1: 0.5863 - Loss: 0.9843\n",
      "\n",
      "Batch 38/348 ━━━━━━━━━━━━━━━━━━━━ 00:35:55\n",
      "Accuracy: 0.9738 - Precision: 0.5215 - Recall: 0.8514 - Specificity: 0.9745 - F1: 0.5898 - Loss: 0.9801\n",
      "\n",
      "Batch 39/348 ━━━━━━━━━━━━━━━━━━━━ 00:37:35\n",
      "Accuracy: 0.9744 - Precision: 0.5325 - Recall: 0.8543 - Specificity: 0.9752 - F1: 0.5993 - Loss: 0.9684\n",
      "\n",
      "Batch 40/348 ━━━━━━━━━━━━━━━━━━━━ 00:39:09\n",
      "Accuracy: 0.9750 - Precision: 0.5416 - Recall: 0.8563 - Specificity: 0.9757 - F1: 0.6072 - Loss: 0.9590\n",
      "\n",
      "Batch 41/348 ━━━━━━━━━━━━━━━━━━━━ 00:40:37\n",
      "Accuracy: 0.9756 - Precision: 0.5497 - Recall: 0.8591 - Specificity: 0.9763 - F1: 0.6148 - Loss: 0.9510\n",
      "\n",
      "Batch 42/348 ━━━━━━━━━━━━━━━━━━━━ 00:42:09\n",
      "Accuracy: 0.9759 - Precision: 0.5486 - Recall: 0.8548 - Specificity: 0.9767 - F1: 0.6139 - Loss: 0.9482\n",
      "\n",
      "Batch 43/348 ━━━━━━━━━━━━━━━━━━━━ 00:43:56\n",
      "Accuracy: 0.9763 - Precision: 0.5446 - Recall: 0.8571 - Specificity: 0.9771 - F1: 0.6122 - Loss: 0.9461\n",
      "\n",
      "Batch 44/348 ━━━━━━━━━━━━━━━━━━━━ 00:45:31\n",
      "Accuracy: 0.9768 - Precision: 0.5499 - Recall: 0.8592 - Specificity: 0.9775 - F1: 0.6177 - Loss: 0.9391\n",
      "\n",
      "Batch 45/348 ━━━━━━━━━━━━━━━━━━━━ 00:47:05\n",
      "Accuracy: 0.9772 - Precision: 0.5596 - Recall: 0.8583 - Specificity: 0.9780 - F1: 0.6238 - Loss: 0.9290\n",
      "\n",
      "Batch 46/348 ━━━━━━━━━━━━━━━━━━━━ 00:48:36\n",
      "Accuracy: 0.9776 - Precision: 0.5636 - Recall: 0.8578 - Specificity: 0.9784 - F1: 0.6274 - Loss: 0.9246\n",
      "\n",
      "Batch 47/348 ━━━━━━━━━━━━━━━━━━━━ 00:50:01\n",
      "Accuracy: 0.9779 - Precision: 0.5676 - Recall: 0.8606 - Specificity: 0.9788 - F1: 0.6322 - Loss: 0.9160\n",
      "\n",
      "Batch 48/348 ━━━━━━━━━━━━━━━━━━━━ 00:51:41\n",
      "Accuracy: 0.9783 - Precision: 0.5687 - Recall: 0.8635 - Specificity: 0.9791 - F1: 0.6350 - Loss: 0.9106\n",
      "\n",
      "Batch 49/348 ━━━━━━━━━━━━━━━━━━━━ 00:53:15\n",
      "Accuracy: 0.9782 - Precision: 0.5587 - Recall: 0.8661 - Specificity: 0.9790 - F1: 0.6251 - Loss: 0.9131\n",
      "\n",
      "Batch 50/348 ━━━━━━━━━━━━━━━━━━━━ 00:54:51\n",
      "Accuracy: 0.9785 - Precision: 0.5633 - Recall: 0.8670 - Specificity: 0.9793 - F1: 0.6295 - Loss: 0.9075\n",
      "\n",
      "Batch 51/348 ━━━━━━━━━━━━━━━━━━━━ 00:56:29\n",
      "Accuracy: 0.9789 - Precision: 0.5709 - Recall: 0.8645 - Specificity: 0.9797 - F1: 0.6335 - Loss: 0.9036\n",
      "\n",
      "Batch 52/348 ━━━━━━━━━━━━━━━━━━━━ 00:58:03\n",
      "Accuracy: 0.9793 - Precision: 0.5772 - Recall: 0.8661 - Specificity: 0.9801 - F1: 0.6390 - Loss: 0.8972\n",
      "\n",
      "Batch 53/348 ━━━━━━━━━━━━━━━━━━━━ 00:59:45\n",
      "Accuracy: 0.9795 - Precision: 0.5826 - Recall: 0.8622 - Specificity: 0.9804 - F1: 0.6411 - Loss: 0.8918\n",
      "\n",
      "Batch 54/348 ━━━━━━━━━━━━━━━━━━━━ 01:01:17\n",
      "Accuracy: 0.9796 - Precision: 0.5794 - Recall: 0.8642 - Specificity: 0.9806 - F1: 0.6398 - Loss: 0.8890\n",
      "\n",
      "Batch 55/348 ━━━━━━━━━━━━━━━━━━━━ 01:02:48\n",
      "Accuracy: 0.9799 - Precision: 0.5819 - Recall: 0.8661 - Specificity: 0.9808 - F1: 0.6432 - Loss: 0.8832\n",
      "\n",
      "Batch 56/348 ━━━━━━━━━━━━━━━━━━━━ 01:04:33\n",
      "Accuracy: 0.9802 - Precision: 0.5826 - Recall: 0.8678 - Specificity: 0.9811 - F1: 0.6452 - Loss: 0.8810\n",
      "\n",
      "Batch 57/348 ━━━━━━━━━━━━━━━━━━━━ 01:06:15\n",
      "Accuracy: 0.9805 - Precision: 0.5893 - Recall: 0.8663 - Specificity: 0.9814 - F1: 0.6490 - Loss: 0.8732\n",
      "\n",
      "Batch 58/348 ━━━━━━━━━━━━━━━━━━━━ 01:07:53\n",
      "Accuracy: 0.9808 - Precision: 0.5918 - Recall: 0.8680 - Specificity: 0.9817 - F1: 0.6522 - Loss: 0.8701\n",
      "\n",
      "Batch 59/348 ━━━━━━━━━━━━━━━━━━━━ 01:09:31\n",
      "Accuracy: 0.9810 - Precision: 0.5948 - Recall: 0.8697 - Specificity: 0.9820 - F1: 0.6557 - Loss: 0.8640\n",
      "\n",
      "Batch 60/348 ━━━━━━━━━━━━━━━━━━━━ 01:11:13\n",
      "Accuracy: 0.9812 - Precision: 0.5947 - Recall: 0.8690 - Specificity: 0.9821 - F1: 0.6562 - Loss: 0.8600\n",
      "\n",
      "Batch 61/348 ━━━━━━━━━━━━━━━━━━━━ 01:12:54\n",
      "Accuracy: 0.9814 - Precision: 0.6003 - Recall: 0.8703 - Specificity: 0.9824 - F1: 0.6609 - Loss: 0.8518\n",
      "\n",
      "Batch 62/348 ━━━━━━━━━━━━━━━━━━━━ 01:14:43\n",
      "Accuracy: 0.9816 - Precision: 0.6024 - Recall: 0.8715 - Specificity: 0.9826 - F1: 0.6635 - Loss: 0.8463\n",
      "\n",
      "Batch 63/348 ━━━━━━━━━━━━━━━━━━━━ 01:16:18\n",
      "Accuracy: 0.9818 - Precision: 0.6084 - Recall: 0.8702 - Specificity: 0.9829 - F1: 0.6669 - Loss: 0.8383\n",
      "\n",
      "Batch 64/348 ━━━━━━━━━━━━━━━━━━━━ 01:17:55\n",
      "Accuracy: 0.9820 - Precision: 0.6091 - Recall: 0.8709 - Specificity: 0.9830 - F1: 0.6683 - Loss: 0.8337\n",
      "\n",
      "Batch 65/348 ━━━━━━━━━━━━━━━━━━━━ 01:19:31\n",
      "Accuracy: 0.9821 - Precision: 0.6062 - Recall: 0.8727 - Specificity: 0.9832 - F1: 0.6672 - Loss: 0.8317\n",
      "\n",
      "Batch 66/348 ━━━━━━━━━━━━━━━━━━━━ 01:21:12\n",
      "Accuracy: 0.9823 - Precision: 0.6083 - Recall: 0.8732 - Specificity: 0.9834 - F1: 0.6694 - Loss: 0.8281\n",
      "\n",
      "Batch 67/348 ━━━━━━━━━━━━━━━━━━━━ 01:22:48\n",
      "Accuracy: 0.9825 - Precision: 0.6137 - Recall: 0.8723 - Specificity: 0.9836 - F1: 0.6726 - Loss: 0.8211\n",
      "\n",
      "Batch 68/348 ━━━━━━━━━━━━━━━━━━━━ 01:24:39\n",
      "Accuracy: 0.9827 - Precision: 0.6149 - Recall: 0.8730 - Specificity: 0.9838 - F1: 0.6744 - Loss: 0.8176\n",
      "\n",
      "Batch 69/348 ━━━━━━━━━━━━━━━━━━━━ 01:26:30\n",
      "Accuracy: 0.9829 - Precision: 0.6194 - Recall: 0.8738 - Specificity: 0.9840 - F1: 0.6780 - Loss: 0.8123\n",
      "\n",
      "Batch 70/348 ━━━━━━━━━━━━━━━━━━━━ 01:28:26\n",
      "Accuracy: 0.9831 - Precision: 0.6217 - Recall: 0.8748 - Specificity: 0.9842 - F1: 0.6805 - Loss: 0.8087\n",
      "\n",
      "Batch 71/348 ━━━━━━━━━━━━━━━━━━━━ 01:30:05\n",
      "Accuracy: 0.9832 - Precision: 0.6197 - Recall: 0.8727 - Specificity: 0.9843 - F1: 0.6791 - Loss: 0.8072\n",
      "\n",
      "Batch 72/348 ━━━━━━━━━━━━━━━━━━━━ 01:31:50\n",
      "Accuracy: 0.9834 - Precision: 0.6232 - Recall: 0.8741 - Specificity: 0.9845 - F1: 0.6824 - Loss: 0.8021\n",
      "\n",
      "Batch 73/348 ━━━━━━━━━━━━━━━━━━━━ 01:33:25\n",
      "Accuracy: 0.9836 - Precision: 0.6233 - Recall: 0.8755 - Specificity: 0.9847 - F1: 0.6836 - Loss: 0.8009\n",
      "\n",
      "Batch 74/348 ━━━━━━━━━━━━━━━━━━━━ 01:35:04\n",
      "Accuracy: 0.9838 - Precision: 0.6265 - Recall: 0.8763 - Specificity: 0.9849 - F1: 0.6864 - Loss: 0.7956\n",
      "\n",
      "Batch 75/348 ━━━━━━━━━━━━━━━━━━━━ 01:36:43\n",
      "Accuracy: 0.9840 - Precision: 0.6289 - Recall: 0.8766 - Specificity: 0.9851 - F1: 0.6886 - Loss: 0.7910\n",
      "\n",
      "Batch 76/348 ━━━━━━━━━━━━━━━━━━━━ 01:38:17\n",
      "Accuracy: 0.9841 - Precision: 0.6337 - Recall: 0.8756 - Specificity: 0.9853 - F1: 0.6913 - Loss: 0.7856\n",
      "\n",
      "Batch 77/348 ━━━━━━━━━━━━━━━━━━━━ 01:39:57\n",
      "Accuracy: 0.9842 - Precision: 0.6363 - Recall: 0.8720 - Specificity: 0.9854 - F1: 0.6913 - Loss: 0.7824\n",
      "\n",
      "Batch 78/348 ━━━━━━━━━━━━━━━━━━━━ 01:41:36\n",
      "Accuracy: 0.9842 - Precision: 0.6312 - Recall: 0.8701 - Specificity: 0.9854 - F1: 0.6870 - Loss: 0.7829\n",
      "\n",
      "Batch 79/348 ━━━━━━━━━━━━━━━━━━━━ 01:43:16\n",
      "Accuracy: 0.9843 - Precision: 0.6257 - Recall: 0.8717 - Specificity: 0.9855 - F1: 0.6825 - Loss: 0.7839\n",
      "\n",
      "Batch 80/348 ━━━━━━━━━━━━━━━━━━━━ 01:44:52\n",
      "Accuracy: 0.9844 - Precision: 0.6272 - Recall: 0.8727 - Specificity: 0.9856 - F1: 0.6844 - Loss: 0.7800\n",
      "\n",
      "Batch 81/348 ━━━━━━━━━━━━━━━━━━━━ 01:46:24\n",
      "Accuracy: 0.9845 - Precision: 0.6289 - Recall: 0.8712 - Specificity: 0.9857 - F1: 0.6853 - Loss: 0.7770\n",
      "\n",
      "Batch 82/348 ━━━━━━━━━━━━━━━━━━━━ 01:48:04\n",
      "Accuracy: 0.9847 - Precision: 0.6329 - Recall: 0.8696 - Specificity: 0.9859 - F1: 0.6871 - Loss: 0.7745\n",
      "\n",
      "Batch 83/348 ━━━━━━━━━━━━━━━━━━━━ 01:49:45\n",
      "Accuracy: 0.9848 - Precision: 0.6367 - Recall: 0.8696 - Specificity: 0.9861 - F1: 0.6898 - Loss: 0.7689\n",
      "\n",
      "Batch 84/348 ━━━━━━━━━━━━━━━━━━━━ 01:51:27\n",
      "Accuracy: 0.9850 - Precision: 0.6408 - Recall: 0.8691 - Specificity: 0.9862 - F1: 0.6922 - Loss: 0.7633\n",
      "\n",
      "Batch 85/348 ━━━━━━━━━━━━━━━━━━━━ 01:53:06\n",
      "Accuracy: 0.9851 - Precision: 0.6431 - Recall: 0.8694 - Specificity: 0.9863 - F1: 0.6942 - Loss: 0.7581\n",
      "\n",
      "Batch 86/348 ━━━━━━━━━━━━━━━━━━━━ 01:54:37\n",
      "Accuracy: 0.9852 - Precision: 0.6447 - Recall: 0.8706 - Specificity: 0.9864 - F1: 0.6963 - Loss: 0.7529\n",
      "\n",
      "Batch 87/348 ━━━━━━━━━━━━━━━━━━━━ 01:56:09\n",
      "Accuracy: 0.9852 - Precision: 0.6423 - Recall: 0.8721 - Specificity: 0.9864 - F1: 0.6952 - Loss: 0.7512\n",
      "\n",
      "Batch 88/348 ━━━━━━━━━━━━━━━━━━━━ 01:57:42\n",
      "Accuracy: 0.9853 - Precision: 0.6449 - Recall: 0.8729 - Specificity: 0.9866 - F1: 0.6977 - Loss: 0.7457\n",
      "\n",
      "Batch 89/348 ━━━━━━━━━━━━━━━━━━━━ 01:59:20\n",
      "Accuracy: 0.9854 - Precision: 0.6485 - Recall: 0.8708 - Specificity: 0.9867 - F1: 0.6989 - Loss: 0.7415\n",
      "\n",
      "Batch 90/348 ━━━━━━━━━━━━━━━━━━━━ 02:00:48\n",
      "Accuracy: 0.9855 - Precision: 0.6506 - Recall: 0.8712 - Specificity: 0.9868 - F1: 0.7007 - Loss: 0.7371\n",
      "\n",
      "Batch 91/348 ━━━━━━━━━━━━━━━━━━━━ 02:02:24\n",
      "Accuracy: 0.9856 - Precision: 0.6513 - Recall: 0.8720 - Specificity: 0.9869 - F1: 0.7020 - Loss: 0.7340\n",
      "\n",
      "Batch 92/348 ━━━━━━━━━━━━━━━━━━━━ 02:04:02\n",
      "Accuracy: 0.9857 - Precision: 0.6524 - Recall: 0.8731 - Specificity: 0.9870 - F1: 0.7036 - Loss: 0.7303\n",
      "\n",
      "Batch 93/348 ━━━━━━━━━━━━━━━━━━━━ 02:05:39\n",
      "Accuracy: 0.9858 - Precision: 0.6544 - Recall: 0.8741 - Specificity: 0.9871 - F1: 0.7056 - Loss: 0.7265\n",
      "\n",
      "Batch 94/348 ━━━━━━━━━━━━━━━━━━━━ 02:07:13\n",
      "Accuracy: 0.9859 - Precision: 0.6569 - Recall: 0.8740 - Specificity: 0.9873 - F1: 0.7075 - Loss: 0.7221\n",
      "\n",
      "Batch 95/348 ━━━━━━━━━━━━━━━━━━━━ 02:08:44\n",
      "Accuracy: 0.9861 - Precision: 0.6599 - Recall: 0.8738 - Specificity: 0.9874 - F1: 0.7094 - Loss: 0.7175\n",
      "\n",
      "Batch 96/348 ━━━━━━━━━━━━━━━━━━━━ 02:10:28\n",
      "Accuracy: 0.9862 - Precision: 0.6621 - Recall: 0.8747 - Specificity: 0.9875 - F1: 0.7116 - Loss: 0.7131\n",
      "\n",
      "Batch 97/348 ━━━━━━━━━━━━━━━━━━━━ 02:12:06\n",
      "Accuracy: 0.9863 - Precision: 0.6649 - Recall: 0.8747 - Specificity: 0.9876 - F1: 0.7136 - Loss: 0.7082\n",
      "\n",
      "Batch 98/348 ━━━━━━━━━━━━━━━━━━━━ 02:13:46\n",
      "Accuracy: 0.9864 - Precision: 0.6657 - Recall: 0.8756 - Specificity: 0.9877 - F1: 0.7149 - Loss: 0.7045\n",
      "\n",
      "Batch 99/348 ━━━━━━━━━━━━━━━━━━━━ 02:15:19\n",
      "Accuracy: 0.9864 - Precision: 0.6624 - Recall: 0.8760 - Specificity: 0.9877 - F1: 0.7126 - Loss: 0.7048\n",
      "\n",
      "Batch 100/348 ━━━━━━━━━━━━━━━━━━━━ 02:17:01\n",
      "Accuracy: 0.9865 - Precision: 0.6596 - Recall: 0.8770 - Specificity: 0.9878 - F1: 0.7110 - Loss: 0.7044\n",
      "\n",
      "Batch 101/348 ━━━━━━━━━━━━━━━━━━━━ 02:18:36\n",
      "Accuracy: 0.9866 - Precision: 0.6625 - Recall: 0.8742 - Specificity: 0.9879 - F1: 0.7111 - Loss: 0.7023\n",
      "\n",
      "Batch 102/348 ━━━━━━━━━━━━━━━━━━━━ 02:20:11\n",
      "Accuracy: 0.9867 - Precision: 0.6646 - Recall: 0.8741 - Specificity: 0.9880 - F1: 0.7127 - Loss: 0.6994\n",
      "\n",
      "Batch 103/348 ━━━━━━━━━━━━━━━━━━━━ 02:21:47\n",
      "Accuracy: 0.9868 - Precision: 0.6674 - Recall: 0.8742 - Specificity: 0.9881 - F1: 0.7147 - Loss: 0.6950\n",
      "\n",
      "Batch 104/348 ━━━━━━━━━━━━━━━━━━━━ 02:23:16\n",
      "Accuracy: 0.9869 - Precision: 0.6701 - Recall: 0.8736 - Specificity: 0.9882 - F1: 0.7162 - Loss: 0.6909\n",
      "\n",
      "Batch 105/348 ━━━━━━━━━━━━━━━━━━━━ 02:24:56\n",
      "Accuracy: 0.9869 - Precision: 0.6711 - Recall: 0.8747 - Specificity: 0.9883 - F1: 0.7177 - Loss: 0.6871\n",
      "\n",
      "Batch 106/348 ━━━━━━━━━━━━━━━━━━━━ 02:26:36\n",
      "Accuracy: 0.9870 - Precision: 0.6737 - Recall: 0.8753 - Specificity: 0.9884 - F1: 0.7198 - Loss: 0.6827\n",
      "\n",
      "Batch 107/348 ━━━━━━━━━━━━━━━━━━━━ 02:28:12\n",
      "Accuracy: 0.9869 - Precision: 0.6689 - Recall: 0.8752 - Specificity: 0.9882 - F1: 0.7156 - Loss: 0.6848\n",
      "\n",
      "Batch 108/348 ━━━━━━━━━━━━━━━━━━━━ 02:29:51\n",
      "Accuracy: 0.9869 - Precision: 0.6658 - Recall: 0.8763 - Specificity: 0.9883 - F1: 0.7137 - Loss: 0.6846\n",
      "\n",
      "Batch 109/348 ━━━━━━━━━━━━━━━━━━━━ 02:31:27\n",
      "Accuracy: 0.9870 - Precision: 0.6672 - Recall: 0.8771 - Specificity: 0.9884 - F1: 0.7153 - Loss: 0.6827\n",
      "\n",
      "Batch 110/348 ━━━━━━━━━━━━━━━━━━━━ 02:33:03\n",
      "Accuracy: 0.9871 - Precision: 0.6681 - Recall: 0.8772 - Specificity: 0.9885 - F1: 0.7162 - Loss: 0.6817\n",
      "\n",
      "Batch 111/348 ━━━━━━━━━━━━━━━━━━━━ 02:34:49\n",
      "Accuracy: 0.9872 - Precision: 0.6710 - Recall: 0.8763 - Specificity: 0.9886 - F1: 0.7176 - Loss: 0.6785\n",
      "\n",
      "Batch 112/348 ━━━━━━━━━━━━━━━━━━━━ 02:36:34\n",
      "Accuracy: 0.9872 - Precision: 0.6739 - Recall: 0.8710 - Specificity: 0.9887 - F1: 0.7152 - Loss: 0.6788\n",
      "\n",
      "Batch 113/348 ━━━━━━━━━━━━━━━━━━━━ 02:38:22\n",
      "Accuracy: 0.9873 - Precision: 0.6765 - Recall: 0.8689 - Specificity: 0.9888 - F1: 0.7156 - Loss: 0.6771\n",
      "\n",
      "Batch 114/348 ━━━━━━━━━━━━━━━━━━━━ 02:39:58\n",
      "Accuracy: 0.9874 - Precision: 0.6786 - Recall: 0.8698 - Specificity: 0.9888 - F1: 0.7175 - Loss: 0.6729\n",
      "\n",
      "Batch 115/348 ━━━━━━━━━━━━━━━━━━━━ 02:41:39\n",
      "Accuracy: 0.9875 - Precision: 0.6789 - Recall: 0.8707 - Specificity: 0.9889 - F1: 0.7185 - Loss: 0.6702\n",
      "\n",
      "Batch 116/348 ━━━━━━━━━━━━━━━━━━━━ 02:43:25\n",
      "Accuracy: 0.9876 - Precision: 0.6793 - Recall: 0.8705 - Specificity: 0.9890 - F1: 0.7190 - Loss: 0.6680\n",
      "\n",
      "Batch 117/348 ━━━━━━━━━━━━━━━━━━━━ 02:45:07\n",
      "Accuracy: 0.9876 - Precision: 0.6798 - Recall: 0.8716 - Specificity: 0.9890 - F1: 0.7202 - Loss: 0.6649\n",
      "\n",
      "Batch 118/348 ━━━━━━━━━━━━━━━━━━━━ 02:46:44\n",
      "Accuracy: 0.9877 - Precision: 0.6774 - Recall: 0.8726 - Specificity: 0.9891 - F1: 0.7188 - Loss: 0.6646\n",
      "\n",
      "Batch 119/348 ━━━━━━━━━━━━━━━━━━━━ 02:48:23\n",
      "Accuracy: 0.9877 - Precision: 0.6793 - Recall: 0.8715 - Specificity: 0.9892 - F1: 0.7196 - Loss: 0.6616\n",
      "\n",
      "Batch 120/348 ━━━━━━━━━━━━━━━━━━━━ 02:49:49\n",
      "Accuracy: 0.9878 - Precision: 0.6811 - Recall: 0.8723 - Specificity: 0.9892 - F1: 0.7214 - Loss: 0.6579\n",
      "\n",
      "Batch 121/348 ━━━━━━━━━━━━━━━━━━━━ 02:51:28\n",
      "Accuracy: 0.9879 - Precision: 0.6783 - Recall: 0.8731 - Specificity: 0.9893 - F1: 0.7195 - Loss: 0.6578\n",
      "\n",
      "Batch 122/348 ━━━━━━━━━━━━━━━━━━━━ 02:52:59\n",
      "Accuracy: 0.9879 - Precision: 0.6778 - Recall: 0.8740 - Specificity: 0.9893 - F1: 0.7199 - Loss: 0.6564\n",
      "\n",
      "Batch 123/348 ━━━━━━━━━━━━━━━━━━━━ 02:54:40\n",
      "Accuracy: 0.9880 - Precision: 0.6760 - Recall: 0.8716 - Specificity: 0.9894 - F1: 0.7182 - Loss: 0.6571\n",
      "\n",
      "Batch 124/348 ━━━━━━━━━━━━━━━━━━━━ 02:56:14\n",
      "Accuracy: 0.9881 - Precision: 0.6785 - Recall: 0.8687 - Specificity: 0.9895 - F1: 0.7179 - Loss: 0.6558\n",
      "\n",
      "Batch 125/348 ━━━━━━━━━━━━━━━━━━━━ 02:57:45\n",
      "Accuracy: 0.9881 - Precision: 0.6809 - Recall: 0.8662 - Specificity: 0.9896 - F1: 0.7178 - Loss: 0.6539\n",
      "\n",
      "Batch 126/348 ━━━━━━━━━━━━━━━━━━━━ 02:59:28\n",
      "Accuracy: 0.9881 - Precision: 0.6805 - Recall: 0.8661 - Specificity: 0.9896 - F1: 0.7178 - Loss: 0.6520\n",
      "\n",
      "Batch 127/348 ━━━━━━━━━━━━━━━━━━━━ 03:01:06\n",
      "Accuracy: 0.9881 - Precision: 0.6784 - Recall: 0.8648 - Specificity: 0.9896 - F1: 0.7163 - Loss: 0.6517\n",
      "\n",
      "Batch 128/348 ━━━━━━━━━━━━━━━━━━━━ 03:02:44\n",
      "Accuracy: 0.9882 - Precision: 0.6784 - Recall: 0.8657 - Specificity: 0.9897 - F1: 0.7170 - Loss: 0.6497\n",
      "\n",
      "Batch 129/348 ━━━━━━━━━━━━━━━━━━━━ 03:04:13\n",
      "Accuracy: 0.9882 - Precision: 0.6755 - Recall: 0.8667 - Specificity: 0.9897 - F1: 0.7150 - Loss: 0.6501\n",
      "\n",
      "Batch 130/348 ━━━━━━━━━━━━━━━━━━━━ 03:05:46\n",
      "Accuracy: 0.9883 - Precision: 0.6765 - Recall: 0.8674 - Specificity: 0.9898 - F1: 0.7162 - Loss: 0.6470\n",
      "\n",
      "Batch 131/348 ━━━━━━━━━━━━━━━━━━━━ 03:07:24\n",
      "Accuracy: 0.9883 - Precision: 0.6760 - Recall: 0.8679 - Specificity: 0.9898 - F1: 0.7164 - Loss: 0.6452\n",
      "\n",
      "Batch 132/348 ━━━━━━━━━━━━━━━━━━━━ 03:08:54\n",
      "Accuracy: 0.9884 - Precision: 0.6782 - Recall: 0.8679 - Specificity: 0.9899 - F1: 0.7179 - Loss: 0.6418\n",
      "\n",
      "Batch 133/348 ━━━━━━━━━━━━━━━━━━━━ 03:10:34\n",
      "Accuracy: 0.9885 - Precision: 0.6803 - Recall: 0.8680 - Specificity: 0.9899 - F1: 0.7194 - Loss: 0.6384\n",
      "\n",
      "Batch 134/348 ━━━━━━━━━━━━━━━━━━━━ 03:12:08\n",
      "Accuracy: 0.9885 - Precision: 0.6816 - Recall: 0.8686 - Specificity: 0.9900 - F1: 0.7207 - Loss: 0.6361\n",
      "\n",
      "Batch 135/348 ━━━━━━━━━━━━━━━━━━━━ 03:13:45\n",
      "Accuracy: 0.9886 - Precision: 0.6826 - Recall: 0.8686 - Specificity: 0.9901 - F1: 0.7216 - Loss: 0.6353\n",
      "\n",
      "Batch 136/348 ━━━━━━━━━━━━━━━━━━━━ 03:15:30\n",
      "Accuracy: 0.9887 - Precision: 0.6835 - Recall: 0.8691 - Specificity: 0.9901 - F1: 0.7227 - Loss: 0.6329\n",
      "\n",
      "Batch 137/348 ━━━━━━━━━━━━━━━━━━━━ 03:17:16\n",
      "Accuracy: 0.9888 - Precision: 0.6857 - Recall: 0.8690 - Specificity: 0.9902 - F1: 0.7241 - Loss: 0.6299\n",
      "\n",
      "Batch 138/348 ━━━━━━━━━━━━━━━━━━━━ 03:18:56\n",
      "Accuracy: 0.9888 - Precision: 0.6878 - Recall: 0.8679 - Specificity: 0.9903 - F1: 0.7248 - Loss: 0.6276\n",
      "\n",
      "Batch 139/348 ━━━━━━━━━━━━━━━━━━━━ 03:20:34\n",
      "Accuracy: 0.9888 - Precision: 0.6898 - Recall: 0.8664 - Specificity: 0.9904 - F1: 0.7253 - Loss: 0.6254\n",
      "\n",
      "Batch 140/348 ━━━━━━━━━━━━━━━━━━━━ 03:22:15\n",
      "Accuracy: 0.9889 - Precision: 0.6905 - Recall: 0.8672 - Specificity: 0.9904 - F1: 0.7263 - Loss: 0.6228\n",
      "\n",
      "Batch 141/348 ━━━━━━━━━━━━━━━━━━━━ 03:23:44\n",
      "Accuracy: 0.9889 - Precision: 0.6876 - Recall: 0.8667 - Specificity: 0.9904 - F1: 0.7242 - Loss: 0.6232\n",
      "\n",
      "Batch 142/348 ━━━━━━━━━━━━━━━━━━━━ 03:25:18\n",
      "Accuracy: 0.9889 - Precision: 0.6881 - Recall: 0.8676 - Specificity: 0.9904 - F1: 0.7251 - Loss: 0.6208\n",
      "\n",
      "Batch 143/348 ━━━━━━━━━━━━━━━━━━━━ 03:26:45\n",
      "Accuracy: 0.9890 - Precision: 0.6898 - Recall: 0.8677 - Specificity: 0.9905 - F1: 0.7264 - Loss: 0.6176\n",
      "\n",
      "Batch 144/348 ━━━━━━━━━━━━━━━━━━━━ 03:28:27\n",
      "Accuracy: 0.9890 - Precision: 0.6905 - Recall: 0.8686 - Specificity: 0.9905 - F1: 0.7275 - Loss: 0.6153\n",
      "\n",
      "Batch 145/348 ━━━━━━━━━━━━━━━━━━━━ 03:30:05\n",
      "Accuracy: 0.9891 - Precision: 0.6911 - Recall: 0.8687 - Specificity: 0.9906 - F1: 0.7281 - Loss: 0.6132\n",
      "\n",
      "Batch 146/348 ━━━━━━━━━━━━━━━━━━━━ 03:31:32\n",
      "Accuracy: 0.9891 - Precision: 0.6929 - Recall: 0.8680 - Specificity: 0.9906 - F1: 0.7290 - Loss: 0.6106\n",
      "\n",
      "Batch 147/348 ━━━━━━━━━━━━━━━━━━━━ 03:33:10\n",
      "Accuracy: 0.9892 - Precision: 0.6948 - Recall: 0.8675 - Specificity: 0.9907 - F1: 0.7300 - Loss: 0.6078\n",
      "\n",
      "Batch 148/348 ━━━━━━━━━━━━━━━━━━━━ 03:34:49\n",
      "Accuracy: 0.9892 - Precision: 0.6964 - Recall: 0.8662 - Specificity: 0.9908 - F1: 0.7303 - Loss: 0.6059\n",
      "\n",
      "Batch 149/348 ━━━━━━━━━━━━━━━━━━━━ 03:36:37\n",
      "Accuracy: 0.9892 - Precision: 0.6933 - Recall: 0.8671 - Specificity: 0.9908 - F1: 0.7280 - Loss: 0.6067\n",
      "\n",
      "Batch 150/348 ━━━━━━━━━━━━━━━━━━━━ 03:38:16\n",
      "Accuracy: 0.9893 - Precision: 0.6931 - Recall: 0.8679 - Specificity: 0.9908 - F1: 0.7285 - Loss: 0.6052\n",
      "\n",
      "Batch 151/348 ━━━━━━━━━━━━━━━━━━━━ 03:39:53\n",
      "Accuracy: 0.9894 - Precision: 0.6944 - Recall: 0.8685 - Specificity: 0.9909 - F1: 0.7297 - Loss: 0.6024\n",
      "\n",
      "Batch 152/348 ━━━━━━━━━━━━━━━━━━━━ 03:41:22\n",
      "Accuracy: 0.9894 - Precision: 0.6923 - Recall: 0.8688 - Specificity: 0.9909 - F1: 0.7284 - Loss: 0.6024\n",
      "\n",
      "Batch 153/348 ━━━━━━━━━━━━━━━━━━━━ 03:42:55\n",
      "Accuracy: 0.9894 - Precision: 0.6924 - Recall: 0.8677 - Specificity: 0.9909 - F1: 0.7282 - Loss: 0.6012\n",
      "\n",
      "Batch 154/348 ━━━━━━━━━━━━━━━━━━━━ 03:44:44\n",
      "Accuracy: 0.9895 - Precision: 0.6944 - Recall: 0.8664 - Specificity: 0.9910 - F1: 0.7287 - Loss: 0.5993\n",
      "\n",
      "Batch 155/348 ━━━━━━━━━━━━━━━━━━━━ 03:46:22\n",
      "Accuracy: 0.9895 - Precision: 0.6963 - Recall: 0.8645 - Specificity: 0.9911 - F1: 0.7287 - Loss: 0.5979\n",
      "\n",
      "Batch 156/348 ━━━━━━━━━━━━━━━━━━━━ 03:48:07\n",
      "Accuracy: 0.9895 - Precision: 0.6980 - Recall: 0.8629 - Specificity: 0.9911 - F1: 0.7288 - Loss: 0.5967\n",
      "\n",
      "Batch 157/348 ━━━━━━━━━━━━━━━━━━━━ 03:49:40\n",
      "Accuracy: 0.9896 - Precision: 0.6971 - Recall: 0.8634 - Specificity: 0.9912 - F1: 0.7286 - Loss: 0.5963\n",
      "\n",
      "Batch 158/348 ━━━━━━━━━━━━━━━━━━━━ 03:51:13\n",
      "Accuracy: 0.9897 - Precision: 0.6982 - Recall: 0.8635 - Specificity: 0.9912 - F1: 0.7295 - Loss: 0.5942\n",
      "\n",
      "Batch 159/348 ━━━━━━━━━━━━━━━━━━━━ 03:52:55\n",
      "Accuracy: 0.9897 - Precision: 0.6946 - Recall: 0.8644 - Specificity: 0.9912 - F1: 0.7264 - Loss: 0.5959\n",
      "\n",
      "Batch 160/348 ━━━━━━━━━━━━━━━━━━━━ 03:54:42\n",
      "Accuracy: 0.9897 - Precision: 0.6953 - Recall: 0.8651 - Specificity: 0.9912 - F1: 0.7274 - Loss: 0.5934\n",
      "\n",
      "Batch 161/348 ━━━━━━━━━━━━━━━━━━━━ 03:56:13\n",
      "Accuracy: 0.9897 - Precision: 0.6929 - Recall: 0.8658 - Specificity: 0.9912 - F1: 0.7258 - Loss: 0.5936\n",
      "\n",
      "Batch 162/348 ━━━━━━━━━━━━━━━━━━━━ 03:57:55\n",
      "Accuracy: 0.9897 - Precision: 0.6928 - Recall: 0.8650 - Specificity: 0.9913 - F1: 0.7257 - Loss: 0.5924\n",
      "\n",
      "Batch 163/348 ━━━━━━━━━━━━━━━━━━━━ 03:59:46\n",
      "Accuracy: 0.9898 - Precision: 0.6945 - Recall: 0.8650 - Specificity: 0.9913 - F1: 0.7269 - Loss: 0.5897\n",
      "\n",
      "Batch 164/348 ━━━━━━━━━━━━━━━━━━━━ 04:01:23\n",
      "Accuracy: 0.9898 - Precision: 0.6963 - Recall: 0.8626 - Specificity: 0.9914 - F1: 0.7263 - Loss: 0.5889\n",
      "\n",
      "Batch 165/348 ━━━━━━━━━━━━━━━━━━━━ 04:03:01\n",
      "Accuracy: 0.9898 - Precision: 0.6971 - Recall: 0.8631 - Specificity: 0.9914 - F1: 0.7273 - Loss: 0.5867\n",
      "\n",
      "Batch 166/348 ━━━━━━━━━━━━━━━━━━━━ 04:04:40\n",
      "Accuracy: 0.9899 - Precision: 0.6961 - Recall: 0.8640 - Specificity: 0.9915 - F1: 0.7271 - Loss: 0.5858\n",
      "\n",
      "Batch 167/348 ━━━━━━━━━━━━━━━━━━━━ 04:06:15\n",
      "Accuracy: 0.9899 - Precision: 0.6968 - Recall: 0.8642 - Specificity: 0.9915 - F1: 0.7279 - Loss: 0.5836\n",
      "\n",
      "Batch 168/348 ━━━━━━━━━━━━━━━━━━━━ 04:07:49\n",
      "Accuracy: 0.9899 - Precision: 0.6983 - Recall: 0.8646 - Specificity: 0.9915 - F1: 0.7291 - Loss: 0.5808\n",
      "\n",
      "Batch 169/348 ━━━━━━━━━━━━━━━━━━━━ 04:09:30\n",
      "Accuracy: 0.9900 - Precision: 0.6985 - Recall: 0.8653 - Specificity: 0.9916 - F1: 0.7298 - Loss: 0.5788\n",
      "\n",
      "Batch 170/348 ━━━━━━━━━━━━━━━━━━━━ 04:11:14\n",
      "Accuracy: 0.9900 - Precision: 0.6996 - Recall: 0.8653 - Specificity: 0.9916 - F1: 0.7306 - Loss: 0.5768\n",
      "\n",
      "Batch 171/348 ━━━━━━━━━━━━━━━━━━━━ 04:13:00\n",
      "Accuracy: 0.9900 - Precision: 0.6979 - Recall: 0.8652 - Specificity: 0.9916 - F1: 0.7296 - Loss: 0.5766\n",
      "\n",
      "Batch 172/348 ━━━━━━━━━━━━━━━━━━━━ 04:14:32\n",
      "Accuracy: 0.9901 - Precision: 0.6991 - Recall: 0.8656 - Specificity: 0.9916 - F1: 0.7307 - Loss: 0.5741\n",
      "\n",
      "Batch 173/348 ━━━━━━━━━━━━━━━━━━━━ 04:16:10\n",
      "Accuracy: 0.9901 - Precision: 0.7006 - Recall: 0.8658 - Specificity: 0.9917 - F1: 0.7319 - Loss: 0.5716\n",
      "\n",
      "Batch 174/348 ━━━━━━━━━━━━━━━━━━━━ 04:17:50\n",
      "Accuracy: 0.9901 - Precision: 0.7000 - Recall: 0.8665 - Specificity: 0.9917 - F1: 0.7319 - Loss: 0.5703\n",
      "\n",
      "Batch 175/348 ━━━━━━━━━━━━━━━━━━━━ 04:19:24\n",
      "Accuracy: 0.9902 - Precision: 0.7013 - Recall: 0.8665 - Specificity: 0.9918 - F1: 0.7329 - Loss: 0.5686\n",
      "\n",
      "Batch 176/348 ━━━━━━━━━━━━━━━━━━━━ 04:21:07\n",
      "Accuracy: 0.9902 - Precision: 0.7026 - Recall: 0.8658 - Specificity: 0.9918 - F1: 0.7334 - Loss: 0.5669\n",
      "\n",
      "Batch 177/348 ━━━━━━━━━━━━━━━━━━━━ 04:22:42\n",
      "Accuracy: 0.9903 - Precision: 0.7034 - Recall: 0.8660 - Specificity: 0.9918 - F1: 0.7342 - Loss: 0.5655\n",
      "\n",
      "Batch 178/348 ━━━━━━━━━━━━━━━━━━━━ 04:24:11\n",
      "Accuracy: 0.9903 - Precision: 0.7051 - Recall: 0.8639 - Specificity: 0.9919 - F1: 0.7338 - Loss: 0.5647\n",
      "\n",
      "Batch 179/348 ━━━━━━━━━━━━━━━━━━━━ 04:25:47\n",
      "Accuracy: 0.9903 - Precision: 0.7066 - Recall: 0.8618 - Specificity: 0.9919 - F1: 0.7333 - Loss: 0.5639\n",
      "\n",
      "Batch 180/348 ━━━━━━━━━━━━━━━━━━━━ 04:27:23\n",
      "Accuracy: 0.9903 - Precision: 0.7070 - Recall: 0.8622 - Specificity: 0.9920 - F1: 0.7339 - Loss: 0.5621\n",
      "\n",
      "Batch 181/348 ━━━━━━━━━━━━━━━━━━━━ 04:28:56\n",
      "Accuracy: 0.9904 - Precision: 0.7077 - Recall: 0.8624 - Specificity: 0.9920 - F1: 0.7347 - Loss: 0.5603\n",
      "\n",
      "Batch 182/348 ━━━━━━━━━━━━━━━━━━━━ 04:30:30\n",
      "Accuracy: 0.9904 - Precision: 0.7089 - Recall: 0.8626 - Specificity: 0.9920 - F1: 0.7357 - Loss: 0.5580\n",
      "\n",
      "Batch 183/348 ━━━━━━━━━━━━━━━━━━━━ 04:32:15\n",
      "Accuracy: 0.9905 - Precision: 0.7094 - Recall: 0.8633 - Specificity: 0.9921 - F1: 0.7365 - Loss: 0.5562\n",
      "\n",
      "Batch 184/348 ━━━━━━━━━━━━━━━━━━━━ 04:33:49\n",
      "Accuracy: 0.9905 - Precision: 0.7105 - Recall: 0.8634 - Specificity: 0.9921 - F1: 0.7374 - Loss: 0.5540\n",
      "\n",
      "Batch 185/348 ━━━━━━━━━━━━━━━━━━━━ 04:35:31\n",
      "Accuracy: 0.9905 - Precision: 0.7103 - Recall: 0.8640 - Specificity: 0.9921 - F1: 0.7377 - Loss: 0.5526\n",
      "\n",
      "Batch 186/348 ━━━━━━━━━━━━━━━━━━━━ 04:37:09\n",
      "Accuracy: 0.9906 - Precision: 0.7113 - Recall: 0.8644 - Specificity: 0.9922 - F1: 0.7387 - Loss: 0.5504\n",
      "\n",
      "Batch 187/348 ━━━━━━━━━━━━━━━━━━━━ 04:38:50\n",
      "Accuracy: 0.9906 - Precision: 0.7124 - Recall: 0.8645 - Specificity: 0.9922 - F1: 0.7395 - Loss: 0.5482\n",
      "\n",
      "Batch 188/348 ━━━━━━━━━━━━━━━━━━━━ 04:40:18\n",
      "Accuracy: 0.9906 - Precision: 0.7124 - Recall: 0.8652 - Specificity: 0.9922 - F1: 0.7400 - Loss: 0.5472\n",
      "\n",
      "Batch 189/348 ━━━━━━━━━━━━━━━━━━━━ 04:42:05\n",
      "Accuracy: 0.9907 - Precision: 0.7121 - Recall: 0.8658 - Specificity: 0.9923 - F1: 0.7402 - Loss: 0.5458\n",
      "\n",
      "Batch 190/348 ━━━━━━━━━━━━━━━━━━━━ 04:43:46\n",
      "Accuracy: 0.9907 - Precision: 0.7133 - Recall: 0.8661 - Specificity: 0.9923 - F1: 0.7413 - Loss: 0.5435\n",
      "\n",
      "Batch 191/348 ━━━━━━━━━━━━━━━━━━━━ 04:45:26\n",
      "Accuracy: 0.9907 - Precision: 0.7148 - Recall: 0.8660 - Specificity: 0.9923 - F1: 0.7421 - Loss: 0.5414\n",
      "\n",
      "Batch 192/348 ━━━━━━━━━━━━━━━━━━━━ 04:46:58\n",
      "Accuracy: 0.9908 - Precision: 0.7160 - Recall: 0.8659 - Specificity: 0.9924 - F1: 0.7429 - Loss: 0.5397\n",
      "\n",
      "Batch 193/348 ━━━━━━━━━━━━━━━━━━━━ 04:48:35\n",
      "Accuracy: 0.9908 - Precision: 0.7168 - Recall: 0.8654 - Specificity: 0.9924 - F1: 0.7433 - Loss: 0.5384\n",
      "\n",
      "Batch 194/348 ━━━━━━━━━━━━━━━━━━━━ 04:50:13\n",
      "Accuracy: 0.9908 - Precision: 0.7178 - Recall: 0.8654 - Specificity: 0.9924 - F1: 0.7441 - Loss: 0.5365\n",
      "\n",
      "Batch 195/348 ━━━━━━━━━━━━━━━━━━━━ 04:52:03\n",
      "Accuracy: 0.9909 - Precision: 0.7173 - Recall: 0.8660 - Specificity: 0.9925 - F1: 0.7441 - Loss: 0.5355\n",
      "\n",
      "Batch 196/348 ━━━━━━━━━━━━━━━━━━━━ 04:53:41\n",
      "Accuracy: 0.9909 - Precision: 0.7184 - Recall: 0.8664 - Specificity: 0.9925 - F1: 0.7451 - Loss: 0.5334\n",
      "\n",
      "Batch 197/348 ━━━━━━━━━━━━━━━━━━━━ 04:55:16\n",
      "Accuracy: 0.9909 - Precision: 0.7196 - Recall: 0.8663 - Specificity: 0.9925 - F1: 0.7459 - Loss: 0.5315\n",
      "\n",
      "Batch 198/348 ━━━━━━━━━━━━━━━━━━━━ 04:57:01\n",
      "Accuracy: 0.9910 - Precision: 0.7206 - Recall: 0.8666 - Specificity: 0.9926 - F1: 0.7468 - Loss: 0.5297\n",
      "\n",
      "Batch 199/348 ━━━━━━━━━━━━━━━━━━━━ 04:58:36\n",
      "Accuracy: 0.9910 - Precision: 0.7219 - Recall: 0.8661 - Specificity: 0.9926 - F1: 0.7474 - Loss: 0.5280\n",
      "\n",
      "Batch 200/348 ━━━━━━━━━━━━━━━━━━━━ 05:00:18\n",
      "Accuracy: 0.9910 - Precision: 0.7212 - Recall: 0.8663 - Specificity: 0.9926 - F1: 0.7471 - Loss: 0.5274\n",
      "\n",
      "Batch 201/348 ━━━━━━━━━━━━━━━━━━━━ 05:02:01\n",
      "Accuracy: 0.9911 - Precision: 0.7220 - Recall: 0.8669 - Specificity: 0.9927 - F1: 0.7481 - Loss: 0.5254\n",
      "\n",
      "Batch 202/348 ━━━━━━━━━━━━━━━━━━━━ 05:03:43\n",
      "Accuracy: 0.9911 - Precision: 0.7229 - Recall: 0.8662 - Specificity: 0.9927 - F1: 0.7483 - Loss: 0.5240\n",
      "\n",
      "Batch 203/348 ━━━━━━━━━━━━━━━━━━━━ 05:05:29\n",
      "Accuracy: 0.9911 - Precision: 0.7237 - Recall: 0.8661 - Specificity: 0.9927 - F1: 0.7490 - Loss: 0.5224\n",
      "\n",
      "Batch 204/348 ━━━━━━━━━━━━━━━━━━━━ 05:07:04\n",
      "Accuracy: 0.9911 - Precision: 0.7243 - Recall: 0.8661 - Specificity: 0.9927 - F1: 0.7495 - Loss: 0.5208\n",
      "\n",
      "Batch 205/348 ━━━━━━━━━━━━━━━━━━━━ 05:08:40\n",
      "Accuracy: 0.9911 - Precision: 0.7227 - Recall: 0.8666 - Specificity: 0.9928 - F1: 0.7486 - Loss: 0.5207\n",
      "\n",
      "Batch 206/348 ━━━━━━━━━━━━━━━━━━━━ 05:10:21\n",
      "Accuracy: 0.9911 - Precision: 0.7198 - Recall: 0.8667 - Specificity: 0.9927 - F1: 0.7461 - Loss: 0.5223\n",
      "\n",
      "Batch 207/348 ━━━━━━━━━━━━━━━━━━━━ 05:12:00\n",
      "Accuracy: 0.9911 - Precision: 0.7203 - Recall: 0.8668 - Specificity: 0.9928 - F1: 0.7466 - Loss: 0.5208\n",
      "\n",
      "Batch 208/348 ━━━━━━━━━━━━━━━━━━━━ 05:14:00\n",
      "Accuracy: 0.9912 - Precision: 0.7214 - Recall: 0.8670 - Specificity: 0.9928 - F1: 0.7475 - Loss: 0.5188\n",
      "\n",
      "Batch 209/348 ━━━━━━━━━━━━━━━━━━━━ 05:15:50\n",
      "Accuracy: 0.9912 - Precision: 0.7226 - Recall: 0.8664 - Specificity: 0.9928 - F1: 0.7479 - Loss: 0.5174\n",
      "\n",
      "Batch 210/348 ━━━━━━━━━━━━━━━━━━━━ 05:17:35\n",
      "Accuracy: 0.9912 - Precision: 0.7238 - Recall: 0.8662 - Specificity: 0.9928 - F1: 0.7486 - Loss: 0.5157\n",
      "\n",
      "Batch 211/348 ━━━━━━━━━━━━━━━━━━━━ 05:19:17\n",
      "Accuracy: 0.9912 - Precision: 0.7237 - Recall: 0.8667 - Specificity: 0.9929 - F1: 0.7489 - Loss: 0.5151\n",
      "\n",
      "Batch 212/348 ━━━━━━━━━━━━━━━━━━━━ 05:21:03\n",
      "Accuracy: 0.9913 - Precision: 0.7250 - Recall: 0.8659 - Specificity: 0.9929 - F1: 0.7492 - Loss: 0.5138\n",
      "\n",
      "Batch 213/348 ━━━━━━━━━━━━━━━━━━━━ 05:22:44\n",
      "Accuracy: 0.9913 - Precision: 0.7262 - Recall: 0.8650 - Specificity: 0.9929 - F1: 0.7495 - Loss: 0.5125\n",
      "\n",
      "Batch 214/348 ━━━━━━━━━━━━━━━━━━━━ 05:24:23\n",
      "Accuracy: 0.9913 - Precision: 0.7268 - Recall: 0.8655 - Specificity: 0.9930 - F1: 0.7502 - Loss: 0.5110\n",
      "\n",
      "Batch 215/348 ━━━━━━━━━━━━━━━━━━━━ 05:25:58\n",
      "Accuracy: 0.9913 - Precision: 0.7254 - Recall: 0.8658 - Specificity: 0.9930 - F1: 0.7495 - Loss: 0.5111\n",
      "\n",
      "Batch 216/348 ━━━━━━━━━━━━━━━━━━━━ 05:27:36\n",
      "Accuracy: 0.9914 - Precision: 0.7256 - Recall: 0.8645 - Specificity: 0.9930 - F1: 0.7491 - Loss: 0.5106\n",
      "\n",
      "Batch 217/348 ━━━━━━━━━━━━━━━━━━━━ 05:29:32\n",
      "Accuracy: 0.9914 - Precision: 0.7236 - Recall: 0.8650 - Specificity: 0.9930 - F1: 0.7478 - Loss: 0.5111\n",
      "\n",
      "Batch 218/348 ━━━━━━━━━━━━━━━━━━━━ 05:31:14\n",
      "Accuracy: 0.9914 - Precision: 0.7245 - Recall: 0.8653 - Specificity: 0.9931 - F1: 0.7486 - Loss: 0.5095\n",
      "\n",
      "Batch 219/348 ━━━━━━━━━━━━━━━━━━━━ 05:32:42\n",
      "Accuracy: 0.9914 - Precision: 0.7258 - Recall: 0.8648 - Specificity: 0.9931 - F1: 0.7491 - Loss: 0.5080\n",
      "\n",
      "Batch 220/348 ━━━━━━━━━━━━━━━━━━━━ 05:34:26\n",
      "Accuracy: 0.9914 - Precision: 0.7270 - Recall: 0.8621 - Specificity: 0.9931 - F1: 0.7476 - Loss: 0.5086\n",
      "\n",
      "Batch 221/348 ━━━━━━━━━━━━━━━━━━━━ 05:35:56\n",
      "Accuracy: 0.9915 - Precision: 0.7282 - Recall: 0.8622 - Specificity: 0.9932 - F1: 0.7485 - Loss: 0.5068\n",
      "\n",
      "Batch 222/348 ━━━━━━━━━━━━━━━━━━━━ 05:37:42\n",
      "Accuracy: 0.9915 - Precision: 0.7293 - Recall: 0.8625 - Specificity: 0.9932 - F1: 0.7494 - Loss: 0.5049\n",
      "\n",
      "Batch 223/348 ━━━━━━━━━━━━━━━━━━━━ 05:39:23\n",
      "Accuracy: 0.9915 - Precision: 0.7283 - Recall: 0.8627 - Specificity: 0.9932 - F1: 0.7490 - Loss: 0.5046\n",
      "\n",
      "Batch 224/348 ━━━━━━━━━━━━━━━━━━━━ 05:41:05\n",
      "Accuracy: 0.9915 - Precision: 0.7293 - Recall: 0.8631 - Specificity: 0.9932 - F1: 0.7499 - Loss: 0.5027\n",
      "\n",
      "Batch 225/348 ━━━━━━━━━━━━━━━━━━━━ 05:42:41\n",
      "Accuracy: 0.9916 - Precision: 0.7304 - Recall: 0.8629 - Specificity: 0.9933 - F1: 0.7505 - Loss: 0.5011\n",
      "\n",
      "Batch 226/348 ━━━━━━━━━━━━━━━━━━━━ 05:44:16\n",
      "Accuracy: 0.9916 - Precision: 0.7308 - Recall: 0.8627 - Specificity: 0.9933 - F1: 0.7508 - Loss: 0.5000\n",
      "\n",
      "Batch 227/348 ━━━━━━━━━━━━━━━━━━━━ 05:45:43\n",
      "Accuracy: 0.9916 - Precision: 0.7318 - Recall: 0.8631 - Specificity: 0.9933 - F1: 0.7517 - Loss: 0.4981\n",
      "\n",
      "Batch 228/348 ━━━━━━━━━━━━━━━━━━━━ 05:47:24\n",
      "Accuracy: 0.9916 - Precision: 0.7315 - Recall: 0.8637 - Specificity: 0.9933 - F1: 0.7520 - Loss: 0.4971\n",
      "\n",
      "Batch 229/348 ━━━━━━━━━━━━━━━━━━━━ 05:49:00\n",
      "Accuracy: 0.9917 - Precision: 0.7316 - Recall: 0.8639 - Specificity: 0.9933 - F1: 0.7523 - Loss: 0.4959\n",
      "\n",
      "Batch 230/348 ━━━━━━━━━━━━━━━━━━━━ 05:50:41\n",
      "Accuracy: 0.9917 - Precision: 0.7305 - Recall: 0.8644 - Specificity: 0.9933 - F1: 0.7518 - Loss: 0.4956\n",
      "\n",
      "Batch 231/348 ━━━━━━━━━━━━━━━━━━━━ 05:52:28\n",
      "Accuracy: 0.9917 - Precision: 0.7316 - Recall: 0.8644 - Specificity: 0.9934 - F1: 0.7526 - Loss: 0.4939\n",
      "\n",
      "Batch 232/348 ━━━━━━━━━━━━━━━━━━━━ 05:53:58\n",
      "Accuracy: 0.9917 - Precision: 0.7323 - Recall: 0.8641 - Specificity: 0.9934 - F1: 0.7529 - Loss: 0.4928\n",
      "\n",
      "Batch 233/348 ━━━━━━━━━━━━━━━━━━━━ 05:55:37\n",
      "Accuracy: 0.9917 - Precision: 0.7323 - Recall: 0.8645 - Specificity: 0.9934 - F1: 0.7532 - Loss: 0.4916\n",
      "\n",
      "Batch 234/348 ━━━━━━━━━━━━━━━━━━━━ 05:57:17\n",
      "Accuracy: 0.9918 - Precision: 0.7332 - Recall: 0.8643 - Specificity: 0.9934 - F1: 0.7538 - Loss: 0.4902\n",
      "\n",
      "Batch 235/348 ━━━━━━━━━━━━━━━━━━━━ 05:58:56\n",
      "Accuracy: 0.9918 - Precision: 0.7343 - Recall: 0.8642 - Specificity: 0.9935 - F1: 0.7545 - Loss: 0.4887\n",
      "\n",
      "Batch 236/348 ━━━━━━━━━━━━━━━━━━━━ 06:00:29\n",
      "Accuracy: 0.9918 - Precision: 0.7331 - Recall: 0.8646 - Specificity: 0.9935 - F1: 0.7538 - Loss: 0.4886\n",
      "\n",
      "Batch 237/348 ━━━━━━━━━━━━━━━━━━━━ 06:02:11\n",
      "Accuracy: 0.9918 - Precision: 0.7341 - Recall: 0.8637 - Specificity: 0.9935 - F1: 0.7539 - Loss: 0.4878\n",
      "\n",
      "Batch 238/348 ━━━━━━━━━━━━━━━━━━━━ 06:03:49\n",
      "Accuracy: 0.9918 - Precision: 0.7342 - Recall: 0.8626 - Specificity: 0.9935 - F1: 0.7535 - Loss: 0.4874\n",
      "\n",
      "Batch 239/348 ━━━━━━━━━━━━━━━━━━━━ 06:05:30\n",
      "Accuracy: 0.9918 - Precision: 0.7352 - Recall: 0.8622 - Specificity: 0.9935 - F1: 0.7540 - Loss: 0.4861\n",
      "\n",
      "Batch 240/348 ━━━━━━━━━━━━━━━━━━━━ 06:07:12\n",
      "Accuracy: 0.9919 - Precision: 0.7360 - Recall: 0.8624 - Specificity: 0.9936 - F1: 0.7546 - Loss: 0.4846\n",
      "\n",
      "Batch 241/348 ━━━━━━━━━━━━━━━━━━━━ 06:08:40\n",
      "Accuracy: 0.9919 - Precision: 0.7346 - Recall: 0.8610 - Specificity: 0.9936 - F1: 0.7534 - Loss: 0.4851\n",
      "\n",
      "Batch 242/348 ━━━━━━━━━━━━━━━━━━━━ 06:10:21\n",
      "Accuracy: 0.9919 - Precision: 0.7345 - Recall: 0.8609 - Specificity: 0.9936 - F1: 0.7535 - Loss: 0.4844\n",
      "\n",
      "Batch 243/348 ━━━━━━━━━━━━━━━━━━━━ 06:11:59\n",
      "Accuracy: 0.9919 - Precision: 0.7356 - Recall: 0.8602 - Specificity: 0.9936 - F1: 0.7537 - Loss: 0.4834\n",
      "\n",
      "Batch 244/348 ━━━━━━━━━━━━━━━━━━━━ 06:13:36\n",
      "Accuracy: 0.9919 - Precision: 0.7364 - Recall: 0.8597 - Specificity: 0.9936 - F1: 0.7540 - Loss: 0.4824\n",
      "\n",
      "Batch 245/348 ━━━━━━━━━━━━━━━━━━━━ 06:15:13\n",
      "Accuracy: 0.9919 - Precision: 0.7370 - Recall: 0.8596 - Specificity: 0.9937 - F1: 0.7545 - Loss: 0.4811\n",
      "\n",
      "Batch 246/348 ━━━━━━━━━━━━━━━━━━━━ 06:16:55\n",
      "Accuracy: 0.9920 - Precision: 0.7373 - Recall: 0.8600 - Specificity: 0.9937 - F1: 0.7550 - Loss: 0.4799\n",
      "\n",
      "Batch 247/348 ━━━━━━━━━━━━━━━━━━━━ 06:18:28\n",
      "Accuracy: 0.9920 - Precision: 0.7378 - Recall: 0.8604 - Specificity: 0.9937 - F1: 0.7556 - Loss: 0.4785\n",
      "\n",
      "Batch 248/348 ━━━━━━━━━━━━━━━━━━━━ 06:20:01\n",
      "Accuracy: 0.9920 - Precision: 0.7381 - Recall: 0.8608 - Specificity: 0.9937 - F1: 0.7561 - Loss: 0.4772\n",
      "\n",
      "Batch 249/348 ━━━━━━━━━━━━━━━━━━━━ 06:21:39\n",
      "Accuracy: 0.9920 - Precision: 0.7386 - Recall: 0.8613 - Specificity: 0.9937 - F1: 0.7568 - Loss: 0.4757\n",
      "\n",
      "Batch 250/348 ━━━━━━━━━━━━━━━━━━━━ 06:23:23\n",
      "Accuracy: 0.9920 - Precision: 0.7394 - Recall: 0.8618 - Specificity: 0.9937 - F1: 0.7575 - Loss: 0.4742\n",
      "\n",
      "Batch 251/348 ━━━━━━━━━━━━━━━━━━━━ 06:24:57\n",
      "Accuracy: 0.9921 - Precision: 0.7397 - Recall: 0.8623 - Specificity: 0.9938 - F1: 0.7581 - Loss: 0.4730\n",
      "\n",
      "Batch 252/348 ━━━━━━━━━━━━━━━━━━━━ 06:26:28\n",
      "Accuracy: 0.9921 - Precision: 0.7385 - Recall: 0.8621 - Specificity: 0.9938 - F1: 0.7574 - Loss: 0.4730\n",
      "\n",
      "Batch 253/348 ━━━━━━━━━━━━━━━━━━━━ 06:28:12\n",
      "Accuracy: 0.9921 - Precision: 0.7382 - Recall: 0.8623 - Specificity: 0.9938 - F1: 0.7574 - Loss: 0.4723\n",
      "\n",
      "Batch 254/348 ━━━━━━━━━━━━━━━━━━━━ 06:29:43\n",
      "Accuracy: 0.9921 - Precision: 0.7391 - Recall: 0.8621 - Specificity: 0.9938 - F1: 0.7579 - Loss: 0.4712\n",
      "\n",
      "Batch 255/348 ━━━━━━━━━━━━━━━━━━━━ 06:31:08\n",
      "Accuracy: 0.9922 - Precision: 0.7400 - Recall: 0.8620 - Specificity: 0.9938 - F1: 0.7584 - Loss: 0.4699\n",
      "\n",
      "Batch 256/348 ━━━━━━━━━━━━━━━━━━━━ 06:32:44\n",
      "Accuracy: 0.9922 - Precision: 0.7408 - Recall: 0.8612 - Specificity: 0.9939 - F1: 0.7585 - Loss: 0.4693\n",
      "\n",
      "Batch 257/348 ━━━━━━━━━━━━━━━━━━━━ 06:34:22\n",
      "Accuracy: 0.9922 - Precision: 0.7404 - Recall: 0.8588 - Specificity: 0.9939 - F1: 0.7569 - Loss: 0.4702\n",
      "\n",
      "Batch 258/348 ━━━━━━━━━━━━━━━━━━━━ 06:36:05\n",
      "Accuracy: 0.9922 - Precision: 0.7414 - Recall: 0.8580 - Specificity: 0.9939 - F1: 0.7570 - Loss: 0.4693\n",
      "\n",
      "Batch 259/348 ━━━━━━━━━━━━━━━━━━━━ 06:37:39\n",
      "Accuracy: 0.9922 - Precision: 0.7421 - Recall: 0.8582 - Specificity: 0.9939 - F1: 0.7577 - Loss: 0.4683\n",
      "\n",
      "Batch 260/348 ━━━━━━━━━━━━━━━━━━━━ 06:39:14\n",
      "Accuracy: 0.9923 - Precision: 0.7429 - Recall: 0.8581 - Specificity: 0.9939 - F1: 0.7581 - Loss: 0.4671\n",
      "\n",
      "Batch 261/348 ━━━━━━━━━━━━━━━━━━━━ 06:40:48\n",
      "Accuracy: 0.9923 - Precision: 0.7410 - Recall: 0.8583 - Specificity: 0.9940 - F1: 0.7566 - Loss: 0.4680\n",
      "\n",
      "Batch 262/348 ━━━━━━━━━━━━━━━━━━━━ 06:42:23\n",
      "Accuracy: 0.9923 - Precision: 0.7414 - Recall: 0.8586 - Specificity: 0.9940 - F1: 0.7572 - Loss: 0.4669\n",
      "\n",
      "Batch 263/348 ━━━━━━━━━━━━━━━━━━━━ 06:43:56\n",
      "Accuracy: 0.9923 - Precision: 0.7424 - Recall: 0.8583 - Specificity: 0.9940 - F1: 0.7576 - Loss: 0.4658\n",
      "\n",
      "Batch 264/348 ━━━━━━━━━━━━━━━━━━━━ 06:45:37\n",
      "Accuracy: 0.9923 - Precision: 0.7423 - Recall: 0.8584 - Specificity: 0.9940 - F1: 0.7577 - Loss: 0.4650\n",
      "\n",
      "Batch 265/348 ━━━━━━━━━━━━━━━━━━━━ 06:47:18\n",
      "Accuracy: 0.9923 - Precision: 0.7428 - Recall: 0.8584 - Specificity: 0.9940 - F1: 0.7582 - Loss: 0.4638\n",
      "\n",
      "Batch 266/348 ━━━━━━━━━━━━━━━━━━━━ 06:48:54\n",
      "Accuracy: 0.9924 - Precision: 0.7426 - Recall: 0.8571 - Specificity: 0.9941 - F1: 0.7576 - Loss: 0.4639\n",
      "\n",
      "Batch 267/348 ━━━━━━━━━━━━━━━━━━━━ 06:50:32\n",
      "Accuracy: 0.9924 - Precision: 0.7428 - Recall: 0.8575 - Specificity: 0.9941 - F1: 0.7579 - Loss: 0.4628\n",
      "\n",
      "Batch 268/348 ━━━━━━━━━━━━━━━━━━━━ 06:52:10\n",
      "Accuracy: 0.9924 - Precision: 0.7434 - Recall: 0.8573 - Specificity: 0.9941 - F1: 0.7583 - Loss: 0.4618\n",
      "\n",
      "Batch 269/348 ━━━━━━━━━━━━━━━━━━━━ 06:53:34\n",
      "Accuracy: 0.9924 - Precision: 0.7419 - Recall: 0.8571 - Specificity: 0.9941 - F1: 0.7572 - Loss: 0.4623\n",
      "\n",
      "Batch 270/348 ━━━━━━━━━━━━━━━━━━━━ 06:55:14\n",
      "Accuracy: 0.9924 - Precision: 0.7427 - Recall: 0.8569 - Specificity: 0.9941 - F1: 0.7576 - Loss: 0.4612\n",
      "\n",
      "Batch 271/348 ━━━━━━━━━━━━━━━━━━━━ 06:56:59\n",
      "Accuracy: 0.9924 - Precision: 0.7436 - Recall: 0.8566 - Specificity: 0.9941 - F1: 0.7580 - Loss: 0.4601\n",
      "\n",
      "Batch 272/348 ━━━━━━━━━━━━━━━━━━━━ 06:58:55\n",
      "Accuracy: 0.9924 - Precision: 0.7445 - Recall: 0.8564 - Specificity: 0.9941 - F1: 0.7585 - Loss: 0.4590\n",
      "\n",
      "Batch 273/348 ━━━━━━━━━━━━━━━━━━━━ 07:00:38\n",
      "Accuracy: 0.9924 - Precision: 0.7454 - Recall: 0.8558 - Specificity: 0.9942 - F1: 0.7587 - Loss: 0.4582\n",
      "\n",
      "Batch 274/348 ━━━━━━━━━━━━━━━━━━━━ 07:02:18\n",
      "Accuracy: 0.9924 - Precision: 0.7457 - Recall: 0.8560 - Specificity: 0.9942 - F1: 0.7591 - Loss: 0.4571\n",
      "\n",
      "Batch 275/348 ━━━━━━━━━━━━━━━━━━━━ 07:03:51\n",
      "Accuracy: 0.9924 - Precision: 0.7446 - Recall: 0.8562 - Specificity: 0.9942 - F1: 0.7585 - Loss: 0.4571\n",
      "\n",
      "Batch 276/348 ━━━━━━━━━━━━━━━━━━━━ 07:05:25\n",
      "Accuracy: 0.9925 - Precision: 0.7449 - Recall: 0.8567 - Specificity: 0.9942 - F1: 0.7590 - Loss: 0.4560\n",
      "\n",
      "Batch 277/348 ━━━━━━━━━━━━━━━━━━━━ 07:07:02\n",
      "Accuracy: 0.9925 - Precision: 0.7452 - Recall: 0.8572 - Specificity: 0.9942 - F1: 0.7595 - Loss: 0.4549\n",
      "\n",
      "Batch 278/348 ━━━━━━━━━━━━━━━━━━━━ 07:08:39\n",
      "Accuracy: 0.9925 - Precision: 0.7457 - Recall: 0.8575 - Specificity: 0.9942 - F1: 0.7601 - Loss: 0.4536\n",
      "\n",
      "Batch 279/348 ━━━━━━━━━━━━━━━━━━━━ 07:10:15\n",
      "Accuracy: 0.9925 - Precision: 0.7459 - Recall: 0.8579 - Specificity: 0.9942 - F1: 0.7605 - Loss: 0.4526\n",
      "\n",
      "Batch 280/348 ━━━━━━━━━━━━━━━━━━━━ 07:11:53\n",
      "Accuracy: 0.9925 - Precision: 0.7466 - Recall: 0.8580 - Specificity: 0.9942 - F1: 0.7611 - Loss: 0.4514\n",
      "\n",
      "Batch 281/348 ━━━━━━━━━━━━━━━━━━━━ 07:13:28\n",
      "Accuracy: 0.9926 - Precision: 0.7471 - Recall: 0.8583 - Specificity: 0.9943 - F1: 0.7616 - Loss: 0.4503\n",
      "\n",
      "Batch 282/348 ━━━━━━━━━━━━━━━━━━━━ 07:15:00\n",
      "Accuracy: 0.9926 - Precision: 0.7475 - Recall: 0.8585 - Specificity: 0.9943 - F1: 0.7621 - Loss: 0.4492\n",
      "\n",
      "Batch 283/348 ━━━━━━━━━━━━━━━━━━━━ 07:16:40\n",
      "Accuracy: 0.9926 - Precision: 0.7476 - Recall: 0.8588 - Specificity: 0.9943 - F1: 0.7624 - Loss: 0.4483\n",
      "\n",
      "Batch 284/348 ━━━━━━━━━━━━━━━━━━━━ 07:18:16\n",
      "Accuracy: 0.9926 - Precision: 0.7483 - Recall: 0.8588 - Specificity: 0.9943 - F1: 0.7629 - Loss: 0.4472\n",
      "\n",
      "Batch 285/348 ━━━━━━━━━━━━━━━━━━━━ 07:20:00\n",
      "Accuracy: 0.9926 - Precision: 0.7491 - Recall: 0.8589 - Specificity: 0.9943 - F1: 0.7634 - Loss: 0.4460\n",
      "\n",
      "Batch 286/348 ━━━━━━━━━━━━━━━━━━━━ 07:21:35\n",
      "Accuracy: 0.9927 - Precision: 0.7489 - Recall: 0.8592 - Specificity: 0.9943 - F1: 0.7636 - Loss: 0.4455\n",
      "\n",
      "Batch 287/348 ━━━━━━━━━━━━━━━━━━━━ 07:23:12\n",
      "Accuracy: 0.9927 - Precision: 0.7492 - Recall: 0.8594 - Specificity: 0.9944 - F1: 0.7639 - Loss: 0.4448\n",
      "\n",
      "Batch 288/348 ━━━━━━━━━━━━━━━━━━━━ 07:24:39\n",
      "Accuracy: 0.9927 - Precision: 0.7501 - Recall: 0.8588 - Specificity: 0.9944 - F1: 0.7641 - Loss: 0.4441\n",
      "\n",
      "Batch 289/348 ━━━━━━━━━━━━━━━━━━━━ 07:26:14\n",
      "Accuracy: 0.9927 - Precision: 0.7508 - Recall: 0.8585 - Specificity: 0.9944 - F1: 0.7644 - Loss: 0.4433\n",
      "\n",
      "Batch 290/348 ━━━━━━━━━━━━━━━━━━━━ 07:27:59\n",
      "Accuracy: 0.9927 - Precision: 0.7513 - Recall: 0.8572 - Specificity: 0.9944 - F1: 0.7640 - Loss: 0.4432\n",
      "\n",
      "Batch 291/348 ━━━━━━━━━━━━━━━━━━━━ 07:29:42\n",
      "Accuracy: 0.9928 - Precision: 0.7512 - Recall: 0.8574 - Specificity: 0.9944 - F1: 0.7641 - Loss: 0.4426\n",
      "\n",
      "Batch 292/348 ━━━━━━━━━━━━━━━━━━━━ 07:31:16\n",
      "Accuracy: 0.9928 - Precision: 0.7520 - Recall: 0.8572 - Specificity: 0.9945 - F1: 0.7645 - Loss: 0.4416\n",
      "\n",
      "Batch 293/348 ━━━━━━━━━━━━━━━━━━━━ 07:32:49\n",
      "Accuracy: 0.9928 - Precision: 0.7523 - Recall: 0.8575 - Specificity: 0.9945 - F1: 0.7649 - Loss: 0.4406\n",
      "\n",
      "Batch 294/348 ━━━━━━━━━━━━━━━━━━━━ 07:34:23\n",
      "Accuracy: 0.9928 - Precision: 0.7528 - Recall: 0.8575 - Specificity: 0.9945 - F1: 0.7653 - Loss: 0.4396\n",
      "\n",
      "Batch 295/348 ━━━━━━━━━━━━━━━━━━━━ 07:36:09\n",
      "Accuracy: 0.9928 - Precision: 0.7532 - Recall: 0.8579 - Specificity: 0.9945 - F1: 0.7659 - Loss: 0.4384\n",
      "\n",
      "Batch 296/348 ━━━━━━━━━━━━━━━━━━━━ 07:37:43\n",
      "Accuracy: 0.9928 - Precision: 0.7534 - Recall: 0.8583 - Specificity: 0.9945 - F1: 0.7663 - Loss: 0.4374\n",
      "\n",
      "Batch 297/348 ━━━━━━━━━━━━━━━━━━━━ 07:39:17\n",
      "Accuracy: 0.9928 - Precision: 0.7534 - Recall: 0.8587 - Specificity: 0.9945 - F1: 0.7665 - Loss: 0.4366\n",
      "\n",
      "Batch 298/348 ━━━━━━━━━━━━━━━━━━━━ 07:40:47\n",
      "Accuracy: 0.9928 - Precision: 0.7538 - Recall: 0.8589 - Specificity: 0.9945 - F1: 0.7669 - Loss: 0.4357\n",
      "\n",
      "Batch 299/348 ━━━━━━━━━━━━━━━━━━━━ 07:42:31\n",
      "Accuracy: 0.9928 - Precision: 0.7539 - Recall: 0.8584 - Specificity: 0.9945 - F1: 0.7669 - Loss: 0.4352\n",
      "\n",
      "Batch 300/348 ━━━━━━━━━━━━━━━━━━━━ 07:44:10\n",
      "Accuracy: 0.9929 - Precision: 0.7545 - Recall: 0.8588 - Specificity: 0.9945 - F1: 0.7675 - Loss: 0.4340\n",
      "\n",
      "Batch 301/348 ━━━━━━━━━━━━━━━━━━━━ 07:45:40\n",
      "Accuracy: 0.9929 - Precision: 0.7544 - Recall: 0.8587 - Specificity: 0.9946 - F1: 0.7675 - Loss: 0.4334\n",
      "\n",
      "Batch 302/348 ━━━━━━━━━━━━━━━━━━━━ 07:47:21\n",
      "Accuracy: 0.9929 - Precision: 0.7548 - Recall: 0.8591 - Specificity: 0.9946 - F1: 0.7681 - Loss: 0.4323\n",
      "\n",
      "Batch 303/348 ━━━━━━━━━━━━━━━━━━━━ 07:48:56\n",
      "Accuracy: 0.9929 - Precision: 0.7553 - Recall: 0.8594 - Specificity: 0.9946 - F1: 0.7686 - Loss: 0.4312\n",
      "\n",
      "Batch 304/348 ━━━━━━━━━━━━━━━━━━━━ 07:50:32\n",
      "Accuracy: 0.9929 - Precision: 0.7550 - Recall: 0.8598 - Specificity: 0.9946 - F1: 0.7686 - Loss: 0.4307\n",
      "\n",
      "Batch 305/348 ━━━━━━━━━━━━━━━━━━━━ 07:52:10\n",
      "Accuracy: 0.9929 - Precision: 0.7545 - Recall: 0.8596 - Specificity: 0.9946 - F1: 0.7684 - Loss: 0.4306\n",
      "\n",
      "Batch 306/348 ━━━━━━━━━━━━━━━━━━━━ 07:53:51\n",
      "Accuracy: 0.9930 - Precision: 0.7552 - Recall: 0.8591 - Specificity: 0.9946 - F1: 0.7685 - Loss: 0.4299\n",
      "\n",
      "Batch 307/348 ━━━━━━━━━━━━━━━━━━━━ 07:55:36\n",
      "Accuracy: 0.9930 - Precision: 0.7560 - Recall: 0.8579 - Specificity: 0.9946 - F1: 0.7682 - Loss: 0.4298\n",
      "\n",
      "Batch 308/348 ━━━━━━━━━━━━━━━━━━━━ 07:57:08\n",
      "Accuracy: 0.9930 - Precision: 0.7564 - Recall: 0.8582 - Specificity: 0.9947 - F1: 0.7686 - Loss: 0.4288\n",
      "\n",
      "Batch 309/348 ━━━━━━━━━━━━━━━━━━━━ 07:58:44\n",
      "Accuracy: 0.9930 - Precision: 0.7570 - Recall: 0.8584 - Specificity: 0.9947 - F1: 0.7691 - Loss: 0.4278\n",
      "\n",
      "Batch 310/348 ━━━━━━━━━━━━━━━━━━━━ 09:17:02\n",
      "Accuracy: 0.9930 - Precision: 0.7577 - Recall: 0.8585 - Specificity: 0.9947 - F1: 0.7697 - Loss: 0.4267\n",
      "\n",
      "Batch 311/348 ━━━━━━━━━━━━━━━━━━━━ 09:19:06\n",
      "Accuracy: 0.9930 - Precision: 0.7584 - Recall: 0.8586 - Specificity: 0.9947 - F1: 0.7702 - Loss: 0.4257\n",
      "\n",
      "Batch 312/348 ━━━━━━━━━━━━━━━━━━━━ 09:20:57\n",
      "Accuracy: 0.9931 - Precision: 0.7584 - Recall: 0.8589 - Specificity: 0.9947 - F1: 0.7705 - Loss: 0.4249\n",
      "\n",
      "Batch 313/348 ━━━━━━━━━━━━━━━━━━━━ 09:22:45\n",
      "Accuracy: 0.9931 - Precision: 0.7591 - Recall: 0.8589 - Specificity: 0.9947 - F1: 0.7709 - Loss: 0.4239\n",
      "\n",
      "Batch 314/348 ━━━━━━━━━━━━━━━━━━━━ 09:25:12\n",
      "Accuracy: 0.9931 - Precision: 0.7598 - Recall: 0.8590 - Specificity: 0.9947 - F1: 0.7714 - Loss: 0.4229\n",
      "\n",
      "Batch 315/348 ━━━━━━━━━━━━━━━━━━━━ 09:26:46\n",
      "Accuracy: 0.9931 - Precision: 0.7603 - Recall: 0.8594 - Specificity: 0.9948 - F1: 0.7719 - Loss: 0.4219\n",
      "\n",
      "Batch 316/348 ━━━━━━━━━━━━━━━━━━━━ 09:28:22\n",
      "Accuracy: 0.9931 - Precision: 0.7590 - Recall: 0.8573 - Specificity: 0.9948 - F1: 0.7704 - Loss: 0.4230\n",
      "\n",
      "Batch 317/348 ━━━━━━━━━━━━━━━━━━━━ 09:30:09\n",
      "Accuracy: 0.9931 - Precision: 0.7592 - Recall: 0.8566 - Specificity: 0.9948 - F1: 0.7701 - Loss: 0.4227\n",
      "\n",
      "Batch 318/348 ━━━━━━━━━━━━━━━━━━━━ 09:31:53\n",
      "Accuracy: 0.9931 - Precision: 0.7588 - Recall: 0.8570 - Specificity: 0.9948 - F1: 0.7701 - Loss: 0.4223\n",
      "\n",
      "Batch 319/348 ━━━━━━━━━━━━━━━━━━━━ 09:33:40\n",
      "Accuracy: 0.9931 - Precision: 0.7574 - Recall: 0.8571 - Specificity: 0.9948 - F1: 0.7692 - Loss: 0.4228\n",
      "\n",
      "Batch 320/348 ━━━━━━━━━━━━━━━━━━━━ 09:35:11\n",
      "Accuracy: 0.9931 - Precision: 0.7581 - Recall: 0.8572 - Specificity: 0.9948 - F1: 0.7698 - Loss: 0.4217\n",
      "\n",
      "Batch 321/348 ━━━━━━━━━━━━━━━━━━━━ 09:37:02\n",
      "Accuracy: 0.9931 - Precision: 0.7588 - Recall: 0.8574 - Specificity: 0.9948 - F1: 0.7703 - Loss: 0.4207\n",
      "\n",
      "Batch 322/348 ━━━━━━━━━━━━━━━━━━━━ 09:38:46\n",
      "Accuracy: 0.9932 - Precision: 0.7594 - Recall: 0.8577 - Specificity: 0.9948 - F1: 0.7709 - Loss: 0.4196\n",
      "\n",
      "Batch 323/348 ━━━━━━━━━━━━━━━━━━━━ 09:40:22\n",
      "Accuracy: 0.9932 - Precision: 0.7599 - Recall: 0.8577 - Specificity: 0.9948 - F1: 0.7712 - Loss: 0.4187\n",
      "\n",
      "Batch 324/348 ━━━━━━━━━━━━━━━━━━━━ 09:41:57\n",
      "Accuracy: 0.9932 - Precision: 0.7605 - Recall: 0.8576 - Specificity: 0.9948 - F1: 0.7716 - Loss: 0.4178\n",
      "\n",
      "Batch 325/348 ━━━━━━━━━━━━━━━━━━━━ 09:43:40\n",
      "Accuracy: 0.9932 - Precision: 0.7609 - Recall: 0.8576 - Specificity: 0.9949 - F1: 0.7719 - Loss: 0.4171\n",
      "\n",
      "Batch 326/348 ━━━━━━━━━━━━━━━━━━━━ 09:45:23\n",
      "Accuracy: 0.9932 - Precision: 0.7615 - Recall: 0.8578 - Specificity: 0.9949 - F1: 0.7724 - Loss: 0.4161\n",
      "\n",
      "Batch 327/348 ━━━━━━━━━━━━━━━━━━━━ 09:46:59\n",
      "Accuracy: 0.9932 - Precision: 0.7613 - Recall: 0.8577 - Specificity: 0.9949 - F1: 0.7724 - Loss: 0.4157\n",
      "\n",
      "Batch 328/348 ━━━━━━━━━━━━━━━━━━━━ 09:48:26\n",
      "Accuracy: 0.9932 - Precision: 0.7616 - Recall: 0.8580 - Specificity: 0.9949 - F1: 0.7728 - Loss: 0.4148\n",
      "\n",
      "Batch 329/348 ━━━━━━━━━━━━━━━━━━━━ 09:50:00\n",
      "Accuracy: 0.9932 - Precision: 0.7613 - Recall: 0.8583 - Specificity: 0.9949 - F1: 0.7728 - Loss: 0.4143\n",
      "\n",
      "Batch 330/348 ━━━━━━━━━━━━━━━━━━━━ 09:51:30\n",
      "Accuracy: 0.9932 - Precision: 0.7610 - Recall: 0.8586 - Specificity: 0.9949 - F1: 0.7729 - Loss: 0.4138\n",
      "\n",
      "Batch 331/348 ━━━━━━━━━━━━━━━━━━━━ 09:53:08\n",
      "Accuracy: 0.9932 - Precision: 0.7613 - Recall: 0.8590 - Specificity: 0.9949 - F1: 0.7733 - Loss: 0.4130\n",
      "\n",
      "Batch 332/348 ━━━━━━━━━━━━━━━━━━━━ 09:54:55\n",
      "Accuracy: 0.9932 - Precision: 0.7619 - Recall: 0.8591 - Specificity: 0.9949 - F1: 0.7737 - Loss: 0.4120\n",
      "\n",
      "Batch 333/348 ━━━━━━━━━━━━━━━━━━━━ 09:56:41\n",
      "Accuracy: 0.9932 - Precision: 0.7626 - Recall: 0.8588 - Specificity: 0.9949 - F1: 0.7740 - Loss: 0.4113\n",
      "\n",
      "Batch 334/348 ━━━━━━━━━━━━━━━━━━━━ 09:58:25\n",
      "Accuracy: 0.9932 - Precision: 0.7608 - Recall: 0.8583 - Specificity: 0.9949 - F1: 0.7726 - Loss: 0.4124\n",
      "\n",
      "Batch 335/348 ━━━━━━━━━━━━━━━━━━━━ 09:59:59\n",
      "Accuracy: 0.9932 - Precision: 0.7603 - Recall: 0.8567 - Specificity: 0.9949 - F1: 0.7714 - Loss: 0.4132\n",
      "\n",
      "Batch 336/348 ━━━━━━━━━━━━━━━━━━━━ 10:01:52\n",
      "Accuracy: 0.9932 - Precision: 0.7610 - Recall: 0.8558 - Specificity: 0.9949 - F1: 0.7713 - Loss: 0.4130\n",
      "\n",
      "Batch 337/348 ━━━━━━━━━━━━━━━━━━━━ 10:03:34\n",
      "Accuracy: 0.9932 - Precision: 0.7616 - Recall: 0.8559 - Specificity: 0.9949 - F1: 0.7717 - Loss: 0.4121\n",
      "\n",
      "Batch 338/348 ━━━━━━━━━━━━━━━━━━━━ 10:05:11\n",
      "Accuracy: 0.9932 - Precision: 0.7601 - Recall: 0.8560 - Specificity: 0.9949 - F1: 0.7706 - Loss: 0.4127\n",
      "\n",
      "Batch 339/348 ━━━━━━━━━━━━━━━━━━━━ 10:06:48\n",
      "Accuracy: 0.9932 - Precision: 0.7608 - Recall: 0.8550 - Specificity: 0.9949 - F1: 0.7703 - Loss: 0.4127\n",
      "\n",
      "Batch 340/348 ━━━━━━━━━━━━━━━━━━━━ 10:08:31\n",
      "Accuracy: 0.9932 - Precision: 0.7614 - Recall: 0.8546 - Specificity: 0.9949 - F1: 0.7705 - Loss: 0.4121\n",
      "\n",
      "Batch 341/348 ━━━━━━━━━━━━━━━━━━━━ 10:10:07\n",
      "Accuracy: 0.9932 - Precision: 0.7613 - Recall: 0.8546 - Specificity: 0.9950 - F1: 0.7705 - Loss: 0.4117\n",
      "\n",
      "Batch 342/348 ━━━━━━━━━━━━━━━━━━━━ 10:11:40\n",
      "Accuracy: 0.9932 - Precision: 0.7619 - Recall: 0.8546 - Specificity: 0.9950 - F1: 0.7709 - Loss: 0.4108\n",
      "\n",
      "Batch 343/348 ━━━━━━━━━━━━━━━━━━━━ 10:13:21\n",
      "Accuracy: 0.9933 - Precision: 0.7624 - Recall: 0.8548 - Specificity: 0.9950 - F1: 0.7714 - Loss: 0.4100\n",
      "\n",
      "Batch 344/348 ━━━━━━━━━━━━━━━━━━━━ 10:14:53\n",
      "Accuracy: 0.9933 - Precision: 0.7623 - Recall: 0.8551 - Specificity: 0.9950 - F1: 0.7715 - Loss: 0.4093\n",
      "\n",
      "Batch 345/348 ━━━━━━━━━━━━━━━━━━━━ 10:16:26\n",
      "Accuracy: 0.9932 - Precision: 0.7602 - Recall: 0.8549 - Specificity: 0.9950 - F1: 0.7696 - Loss: 0.4109\n",
      "\n",
      "Batch 346/348 ━━━━━━━━━━━━━━━━━━━━ 10:18:15\n",
      "Accuracy: 0.9933 - Precision: 0.7601 - Recall: 0.8554 - Specificity: 0.9950 - F1: 0.7698 - Loss: 0.4103\n",
      "\n",
      "Batch 347/348 ━━━━━━━━━━━━━━━━━━━━ 10:19:49\n",
      "Accuracy: 0.9933 - Precision: 0.7594 - Recall: 0.8551 - Specificity: 0.9950 - F1: 0.7694 - Loss: 0.4104\n",
      "\n",
      "Batch 348/348 ━━━━━━━━━━━━━━━━━━━━ 10:21:22\n",
      "Accuracy: 0.9933 - Precision: 0.7599 - Recall: 0.8553 - Specificity: 0.9950 - F1: 0.7698 - Loss: 0.4096\n",
      "\n",
      "Validation Set Performance ━━━━━━━━━━━━━━━━━━━━ 10:57:50\n",
      "Accuracy: 0.9946 - Precision: 0.7329 - Recall: 0.9068 - Specificity: 0.7861 - F1: 0.9956 - Loss: 0.2752\n",
      "\n",
      "End of Epoch 1\n",
      "\n",
      "Epoch 2/50\n",
      "Batch 1/348 ━━━━━━━━━━━━━━━━━━━━ 10:59:23\n",
      "Accuracy: 0.9980 - Precision: 0.9794 - Recall: 0.8964 - Specificity: 0.9997 - F1: 0.9361 - Loss: 0.0891\n",
      "\n",
      "Batch 2/348 ━━━━━━━━━━━━━━━━━━━━ 11:00:54\n",
      "Accuracy: 0.9976 - Precision: 0.8164 - Recall: 0.7870 - Specificity: 0.9991 - F1: 0.8007 - Loss: 0.2465\n",
      "\n",
      "Batch 3/348 ━━━━━━━━━━━━━━━━━━━━ 11:02:36\n",
      "Accuracy: 0.9980 - Precision: 0.8190 - Recall: 0.8391 - Specificity: 0.9991 - F1: 0.8270 - Loss: 0.2193\n",
      "\n",
      "Batch 4/348 ━━━━━━━━━━━━━━━━━━━━ 11:04:16\n",
      "Accuracy: 0.9948 - Precision: 0.8642 - Recall: 0.6996 - Specificity: 0.9993 - F1: 0.7300 - Loss: 0.3283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.ndimage import zoom, rotate\n",
    "import random\n",
    "\n",
    "# Paths to the dataset\n",
    "image_dir = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\3D\\Task01_BrainTumour\\imagesTr'\n",
    "label_dir = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\3D\\Task01_BrainTumour\\labelsTr'\n",
    "#image_dir = r'F:\\Data\\3D\\Task01_BrainTumour\\imagesTr'\n",
    "#label_dir = r'F:\\Data\\3D\\Task01_BrainTumour\\labelsTr'\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1  # Reduced batch size to minimize memory load\n",
    "dim = (128, 128, 128)  # Target dimensions for resizing\n",
    "epochs = 50\n",
    "modalities = [\"FLAIR\", \"T1w\", \"t1gd\", \"T2w\"]\n",
    "\n",
    "# Load filenames and split into training, validation, and testing sets\n",
    "image_files = sorted(os.listdir(image_dir))\n",
    "label_files = sorted(os.listdir(label_dir))\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(image_files, label_files, test_size=0.1, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Generator Class with Data Augmentation\n",
    "class NiftiDataset(Sequence):\n",
    "    def __init__(self, image_files, label_files, image_dir, label_dir, batch_size=2, dim=(128, 128, 128), shuffle=True, augment=False):\n",
    "        self.image_files = image_files\n",
    "        self.label_files = label_files\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.indexes = np.arange(len(self.image_files))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # Pre-allocate arrays for images and masks\n",
    "        batch_images = np.zeros((self.batch_size, *self.dim, 4), dtype=np.float32)\n",
    "        batch_masks = np.zeros((self.batch_size, *self.dim, 1), dtype=np.float32)\n",
    "\n",
    "        for i, idx in enumerate(batch_indexes):\n",
    "            img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "            mask_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "\n",
    "            img = nib.load(img_path).get_fdata()  # Load 4D data with modalities as channels\n",
    "            mask = nib.load(mask_path).get_fdata()\n",
    "\n",
    "            img_resized = self.preprocess_image(img)\n",
    "            mask_resized = self.preprocess_mask(mask)\n",
    "\n",
    "            if self.augment:\n",
    "                img_resized, mask_resized = self.augment_data(img_resized, mask_resized)\n",
    "\n",
    "            batch_images[i] = img_resized\n",
    "            batch_masks[i] = mask_resized[..., np.newaxis]  # Add channel dimension\n",
    "\n",
    "        return batch_images, batch_masks\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def preprocess_image(self, img):\n",
    "        img = (img - np.mean(img, axis=(0, 1, 2))) / (np.std(img, axis=(0, 1, 2)) + 1e-7)  # Normalize per modality\n",
    "        zoom_factors = [self.dim[i] / img.shape[i] for i in range(3)] + [1]  # Keep channels dimension unchanged\n",
    "        img_resized = zoom(img, zoom_factors, order=1)\n",
    "        return img_resized\n",
    "\n",
    "    def preprocess_mask(self, mask):\n",
    "        # Binarize the mask: Convert all non-zero labels to 1\n",
    "        mask = np.where(mask > 0, 1, 0)\n",
    "        zoom_factors = [self.dim[i] / mask.shape[i] for i in range(3)]\n",
    "        mask_resized = zoom(mask, zoom_factors, order=0)  # Use nearest-neighbor for binary masks\n",
    "        return mask_resized\n",
    "\n",
    "    def augment_data(self, image, mask):\n",
    "        # Randomly apply data augmentation\n",
    "        # Flip\n",
    "        if random.random() < 0.5:\n",
    "            axis = random.choice([0, 1, 2])\n",
    "            image = np.flip(image, axis=axis)\n",
    "            mask = np.flip(mask, axis=axis)\n",
    "        # Rotation\n",
    "        if random.random() < 0.5:\n",
    "            angle = random.uniform(-10, 10)\n",
    "            axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "            image = rotate(image, angle=angle, axes=axes, reshape=False, order=1)\n",
    "            mask = rotate(mask, angle=angle, axes=axes, reshape=False, order=0)\n",
    "        return image, mask\n",
    "\n",
    "# Define custom loss functions\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-7  # Smoothing constant to prevent division by zero\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    denominator = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f)\n",
    "    dice_coeff = (2. * intersection + smooth) / (denominator + smooth)\n",
    "    return 1 - dice_coeff\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    bce = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred))\n",
    "    d_loss = dice_loss(y_true, y_pred)\n",
    "    return bce + d_loss\n",
    "\n",
    "# Define TensorFlow-based Custom Metrics with Clipping\n",
    "def custom_precision(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    precision = tf.clip_by_value(precision, 0, 1)  # Clip the precision to ensure it stays between 0 and 1\n",
    "    return precision\n",
    "\n",
    "def custom_recall(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    recall = tf.clip_by_value(recall, 0, 1)  # Clip recall between 0 and 1\n",
    "    return recall\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    precision = custom_precision(y_true, y_pred)\n",
    "    recall = custom_recall(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "    f1 = tf.clip_by_value(f1, 0, 1)  # Clip F1 score between 0 and 1\n",
    "    return f1\n",
    "\n",
    "def custom_specificity(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "    tn = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    specificity = tn / (tn + fp + 1e-7)\n",
    "    specificity = tf.clip_by_value(specificity, 0, 1)  # Clip specificity between 0 and 1\n",
    "    return specificity\n",
    "\n",
    "# Define convolutional block with residual connections\n",
    "def conv_block(inputs, filters, kernel_size=(3, 3, 3), padding='same', activation='relu'):\n",
    "    x = tf.keras.layers.Conv3D(filters, kernel_size, padding=padding)(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(activation)(x)\n",
    "    x = tf.keras.layers.Conv3D(filters, kernel_size, padding=padding)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    # Residual connection\n",
    "    shortcut = tf.keras.layers.Conv3D(filters, kernel_size=(1, 1, 1), padding='same')(inputs)\n",
    "    shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "    x = tf.keras.layers.Add()([x, shortcut])\n",
    "    x = tf.keras.layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "# Define 3D U-Net Model with Residual Connections\n",
    "def unet_3d(input_shape=(128, 128, 128, 4)):  # Adjust input_shape for 4 modalities\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = conv_block(inputs, 32)\n",
    "    p1 = tf.keras.layers.MaxPooling3D((2, 2, 2))(c1)\n",
    "\n",
    "    c2 = conv_block(p1, 64)\n",
    "    p2 = tf.keras.layers.MaxPooling3D((2, 2, 2))(c2)\n",
    "\n",
    "    c3 = conv_block(p2, 128)\n",
    "    p3 = tf.keras.layers.MaxPooling3D((2, 2, 2))(c3)\n",
    "\n",
    "    c4 = conv_block(p3, 256)\n",
    "\n",
    "    # Decoder\n",
    "    u5 = tf.keras.layers.UpSampling3D((2, 2, 2))(c4)\n",
    "    u5 = tf.keras.layers.Conv3D(128, (2, 2, 2), activation='relu', padding='same')(u5)\n",
    "    merge5 = tf.keras.layers.concatenate([c3, u5], axis=4)\n",
    "    c5 = conv_block(merge5, 128)\n",
    "\n",
    "    u6 = tf.keras.layers.UpSampling3D((2, 2, 2))(c5)\n",
    "    u6 = tf.keras.layers.Conv3D(64, (2, 2, 2), activation='relu', padding='same')(u6)\n",
    "    merge6 = tf.keras.layers.concatenate([c2, u6], axis=4)\n",
    "    c6 = conv_block(merge6, 64)\n",
    "\n",
    "    u7 = tf.keras.layers.UpSampling3D((2, 2, 2))(c6)\n",
    "    u7 = tf.keras.layers.Conv3D(32, (2, 2, 2), activation='relu', padding='same')(u7)\n",
    "    merge7 = tf.keras.layers.concatenate([c1, u7], axis=4)\n",
    "    c7 = conv_block(merge7, 32)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv3D(1, (1, 1, 1), activation='sigmoid')(c7)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Custom callback to print more metrics at each batch and epoch\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches, val_gen, test_gen):\n",
    "        super().__init__()\n",
    "        self.total_batches = total_batches\n",
    "        self.batch_counter = 1\n",
    "        self.val_gen = val_gen\n",
    "        self.test_gen = test_gen\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.batch_counter = 1  # Reset batch counter at the start of each epoch\n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Batch {self.batch_counter}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Specificity: {specificity:.4f} - F1: {f1:.4f} - Loss: {loss:.4f}\\n\")\n",
    "        self.batch_counter += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_specificity, val_f1 = self.model.evaluate(self.val_gen, verbose=0)\n",
    "        print(f\"Validation Set Performance ━━━━━━━━━━━━━━━━━━━━ {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"Accuracy: {val_accuracy:.4f} - Precision: {val_precision:.4f} - Recall: {val_recall:.4f} - Specificity: {val_specificity:.4f} - F1: {val_f1:.4f} - Loss: {val_loss:.4f}\\n\")\n",
    "        print(f\"End of Epoch {epoch+1}\")\n",
    "\n",
    "# Initialize Generators with Data Augmentation for Training\n",
    "train_gen = NiftiDataset(train_images, train_labels, image_dir, label_dir, batch_size=batch_size, dim=dim, augment=True)\n",
    "val_gen = NiftiDataset(val_images, val_labels, image_dir, label_dir, batch_size=batch_size, dim=dim, shuffle=False)\n",
    "test_gen = NiftiDataset(test_images, test_labels, image_dir, label_dir, batch_size=batch_size, dim=dim, shuffle=False)\n",
    "\n",
    "# Define Model and Compile with Combined Loss Function\n",
    "model = unet_3d(input_shape=dim + (4,))  # Adjust input_shape to include 4 channels for modalities\n",
    "model.compile(optimizer='adam', loss=combined_loss,\n",
    "              metrics=['accuracy', custom_precision, custom_recall, custom_f1, custom_specificity])\n",
    "\n",
    "# Define Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize Metrics Callback\n",
    "steps_per_epoch = len(train_gen)\n",
    "metrics_callback = MetricsCallback(total_batches=steps_per_epoch, val_gen=val_gen, test_gen=test_gen)\n",
    "\n",
    "# Train Model with Data Augmentation and Modified Loss Function\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=[early_stopping, metrics_callback], verbose=0)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_specificity, test_f1 = model.evaluate(test_gen, verbose=0)\n",
    "\n",
    "# Display Test Set Results\n",
    "current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(f\"Test Set Performance Results ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f} - Precision: {test_precision:.4f} - Recall: {test_recall:.4f} - Specificity: {test_specificity:.4f} - F1: {test_f1:.4f} - Loss: {test_loss:.4f}\\n\")\n",
    "\n",
    "# Visualization Function for Multi-Modal Data\n",
    "def visualize_predictions(images, true_masks, pred_masks, title):\n",
    "    slice_index = images.shape[2] // 2\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i in range(4):  # Loop over modalities\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        plt.imshow(images[0][:, :, slice_index, i], cmap='gray')\n",
    "        plt.title(f'Modality: {modalities[i]}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(true_masks[0][:, :, slice_index].squeeze(), cmap='jet', alpha=0.7)\n",
    "    plt.title('Ground Truth Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(pred_masks[0][:, :, slice_index].squeeze(), cmap='jet', alpha=0.7)\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Predictions for Test Set\n",
    "X_test_batch, y_test_batch = val_gen.__getitem__(0)  # Get first batch from validation set\n",
    "y_test_pred_batch = model.predict(X_test_batch)\n",
    "y_test_pred_batch_bin = (y_test_pred_batch > 0.5).astype(np.uint8)\n",
    "\n",
    "visualize_predictions(X_test_batch, y_test_batch, y_test_pred_batch_bin, \"Test Set Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e6b98-9b74-4143-836b-c8c8354be8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
