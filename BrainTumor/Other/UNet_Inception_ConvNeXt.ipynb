{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e799545-013b-4217-914b-e7511be83726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.ndimage import zoom, rotate\n",
    "import random\n",
    "\n",
    "# Paths to the dataset\n",
    "image_dir = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\3D\\Task01_BrainTumour\\imagesTr'\n",
    "label_dir = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\3D\\Task01_BrainTumour\\labelsTr'\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1  # Reduced batch size to minimize memory load\n",
    "dim = (128, 128, 128)  # Target dimensions for resizing\n",
    "epochs = 50\n",
    "modalities = [\"FLAIR\", \"T1w\", \"t1gd\", \"T2w\"]\n",
    "\n",
    "# Load filenames and split into training, validation, and testing sets\n",
    "image_files = sorted(os.listdir(image_dir))\n",
    "label_files = sorted(os.listdir(label_dir))\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(image_files, label_files, test_size=0.1, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Generator Class with Data Augmentation\n",
    "class NiftiDataset(Sequence):\n",
    "    def __init__(self, image_files, label_files, image_dir, label_dir, batch_size=2, dim=(128, 128, 128), shuffle=True, augment=False):\n",
    "        self.image_files = image_files\n",
    "        self.label_files = label_files\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.indexes = np.arange(len(self.image_files))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_images = np.zeros((self.batch_size, *self.dim, 4), dtype=np.float32)\n",
    "        batch_masks = np.zeros((self.batch_size, *self.dim, 1), dtype=np.float32)\n",
    "\n",
    "        for i, idx in enumerate(batch_indexes):\n",
    "            img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "            mask_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "\n",
    "            img = nib.load(img_path).get_fdata()\n",
    "            mask = nib.load(mask_path).get_fdata()\n",
    "\n",
    "            img_resized = self.preprocess_image(img)\n",
    "            mask_resized = self.preprocess_mask(mask)\n",
    "\n",
    "            if self.augment:\n",
    "                img_resized, mask_resized = self.augment_data(img_resized, mask_resized)\n",
    "\n",
    "            batch_images[i] = img_resized\n",
    "            batch_masks[i] = mask_resized[..., np.newaxis]\n",
    "\n",
    "        return batch_images, batch_masks\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def preprocess_image(self, img):\n",
    "        img = (img - np.mean(img, axis=(0, 1, 2))) / (np.std(img, axis=(0, 1, 2)) + 1e-7)\n",
    "        zoom_factors = [self.dim[i] / img.shape[i] for i in range(3)] + [1]\n",
    "        img_resized = zoom(img, zoom_factors, order=1)\n",
    "        return img_resized\n",
    "\n",
    "    def preprocess_mask(self, mask):\n",
    "        mask = np.where(mask > 0, 1, 0)\n",
    "        zoom_factors = [self.dim[i] / mask.shape[i] for i in range(3)]\n",
    "        mask_resized = zoom(mask, zoom_factors, order=0)\n",
    "        return mask_resized\n",
    "\n",
    "    def augment_data(self, image, mask):\n",
    "        if random.random() < 0.5:\n",
    "            axis = random.choice([0, 1, 2])\n",
    "            image = np.flip(image, axis=axis)\n",
    "            mask = np.flip(mask, axis=axis)\n",
    "        if random.random() < 0.5:\n",
    "            angle = random.uniform(-10, 10)\n",
    "            axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "            image = rotate(image, angle=angle, axes=axes, reshape=False, order=1)\n",
    "            mask = rotate(mask, angle=angle, axes=axes, reshape=False, order=0)\n",
    "        return image, mask\n",
    "\n",
    "# Define custom loss functions\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-7\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    denominator = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f)\n",
    "    dice_coeff = (2. * intersection + smooth) / (denominator + smooth)\n",
    "    return 1 - dice_coeff\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    bce = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred))\n",
    "    d_loss = dice_loss(y_true, y_pred)\n",
    "    return bce + d_loss\n",
    "\n",
    "# Define custom metrics\n",
    "def custom_precision(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    return tf.clip_by_value(precision, 0, 1)\n",
    "\n",
    "def custom_recall(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    return tf.clip_by_value(recall, 0, 1)\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    precision = custom_precision(y_true, y_pred)\n",
    "    recall = custom_recall(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "    return tf.clip_by_value(f1, 0, 1)\n",
    "\n",
    "def custom_specificity(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "    tn = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    specificity = tn / (tn + fp + 1e-7)\n",
    "    return tf.clip_by_value(specificity, 0, 1)\n",
    "\n",
    "# Define advanced ConvNeXt block with modified groups\n",
    "def convnext_block(inputs, filters):\n",
    "    groups = filters if inputs.shape[-1] % filters == 0 else 1\n",
    "    x = tf.keras.layers.Conv3D(filters, kernel_size=7, padding='same', groups=groups)(inputs)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Conv3D(filters, kernel_size=1, padding='same', activation='gelu')(x)\n",
    "    return x\n",
    "\n",
    "# Define Inception block\n",
    "def inception_block(inputs, filters):\n",
    "    branch1x1 = tf.keras.layers.Conv3D(filters, (1, 1, 1), padding='same', activation='relu')(inputs)\n",
    "\n",
    "    branch3x3 = tf.keras.layers.Conv3D(filters, (1, 1, 1), padding='same', activation='relu')(inputs)\n",
    "    branch3x3 = tf.keras.layers.Conv3D(filters, (3, 3, 3), padding='same', activation='relu')(branch3x3)\n",
    "\n",
    "    branch5x5 = tf.keras.layers.Conv3D(filters, (1, 1, 1), padding='same', activation='relu')(inputs)\n",
    "    branch5x5 = tf.keras.layers.Conv3D(filters, (5, 5, 5), padding='same', activation='relu')(branch5x5)\n",
    "\n",
    "    branch_pool = tf.keras.layers.MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same')(inputs)\n",
    "    branch_pool = tf.keras.layers.Conv3D(filters, (1, 1, 1), padding='same', activation='relu')(branch_pool)\n",
    "\n",
    "    output = tf.keras.layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1)\n",
    "    return output\n",
    "\n",
    "# Define the integrated U-Net model with ConvNeXt and Inception\n",
    "def integrated_unet(input_shape=(128, 128, 128, 4)):\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "\n",
    "    # ConvNeXt + Inception Encoder\n",
    "    c1 = convnext_block(inputs, 32)\n",
    "    c1 = inception_block(c1, 32)\n",
    "    p1 = tf.keras.layers.MaxPooling3D((2, 2, 2))(c1)\n",
    "\n",
    "    c2 = convnext_block(p1, 64)\n",
    "    c2 = inception_block(c2, 64)\n",
    "    p2 = tf.keras.layers.MaxPooling3D((2, 2, 2))(c2)\n",
    "\n",
    "    c3 = convnext_block(p2, 128)\n",
    "    c3 = inception_block(c3, 128)\n",
    "    p3 = tf.keras.layers.MaxPooling3D((2, 2, 2))(c3)\n",
    "\n",
    "    c4 = convnext_block(p3, 256)\n",
    "    c4 = inception_block(c4, 256)\n",
    "\n",
    "    # Decoder\n",
    "    u5 = tf.keras.layers.UpSampling3D((2, 2, 2))(c4)\n",
    "    u5 = tf.keras.layers.Conv3D(128, (2, 2, 2), activation='relu', padding='same')(u5)\n",
    "    merge5 = tf.keras.layers.concatenate([c3, u5], axis=4)\n",
    "    c5 = convnext_block(merge5, 128)\n",
    "    c5 = inception_block(c5, 128)\n",
    "\n",
    "    u6 = tf.keras.layers.UpSampling3D((2, 2, 2))(c5)\n",
    "    u6 = tf.keras.layers.Conv3D(64, (2, 2, 2), activation='relu', padding='same')(u6)\n",
    "    merge6 = tf.keras.layers.concatenate([c2, u6], axis=4)\n",
    "    c6 = convnext_block(merge6, 64)\n",
    "    c6 = inception_block(c6, 64)\n",
    "\n",
    "    u7 = tf.keras.layers.UpSampling3D((2, 2, 2))(c6)\n",
    "    u7 = tf.keras.layers.Conv3D(32, (2, 2, 2), activation='relu', padding='same')(u7)\n",
    "    merge7 = tf.keras.layers.concatenate([c1, u7], axis=4)\n",
    "    c7 = convnext_block(merge7, 32)\n",
    "    c7 = inception_block(c7, 32)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv3D(1, (1, 1, 1), activation='sigmoid')(c7)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Custom callback to print more metrics at each batch and epoch\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches, val_gen, test_gen):\n",
    "        super().__init__()\n",
    "        self.total_batches = total_batches\n",
    "        self.batch_counter = 1\n",
    "        self.val_gen = val_gen\n",
    "        self.test_gen = test_gen\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.batch_counter = 1  # Reset batch counter at the start of each epoch\n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Batch {self.batch_counter}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Specificity: {specificity:.4f} - F1: {f1:.4f} - Loss: {loss:.4f}\\n\")\n",
    "        self.batch_counter += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_specificity, val_f1 = self.model.evaluate(self.val_gen, verbose=0)\n",
    "        print(f\"Validation Set Performance ━━━━━━━━━━━━━━━━━━━━ {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"Accuracy: {val_accuracy:.4f} - Precision: {val_precision:.4f} - Recall: {val_recall:.4f} - Specificity: {val_specificity:.4f} - F1: {val_f1:.4f} - Loss: {val_loss:.4f}\\n\")\n",
    "        print(f\"End of Epoch {epoch+1}\")\n",
    "\n",
    "# Initialize Generators with Data Augmentation for Training\n",
    "train_gen = NiftiDataset(train_images, train_labels, image_dir, label_dir, batch_size=batch_size, dim=dim, augment=True)\n",
    "val_gen = NiftiDataset(val_images, val_labels, image_dir, label_dir, batch_size=batch_size, dim=dim, shuffle=False)\n",
    "test_gen = NiftiDataset(test_images, test_labels, image_dir, label_dir, batch_size=batch_size, dim=dim, shuffle=False)\n",
    "\n",
    "# Define Model and Compile with Combined Loss Function\n",
    "model = integrated_unet(input_shape=dim + (4,))\n",
    "model.compile(optimizer='adam', loss=combined_loss,\n",
    "              metrics=['accuracy', custom_precision, custom_recall, custom_f1, custom_specificity])\n",
    "\n",
    "# Define Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize Metrics Callback\n",
    "steps_per_epoch = len(train_gen)\n",
    "metrics_callback = MetricsCallback(total_batches=steps_per_epoch, val_gen=val_gen, test_gen=test_gen)\n",
    "\n",
    "# Train Model with Data Augmentation and Modified Loss Function\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=[early_stopping, metrics_callback], verbose=0)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_specificity, test_f1 = model.evaluate(test_gen, verbose=0)\n",
    "\n",
    "# Display Test Set Results\n",
    "current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(f\"Test Set Performance Results ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f} - Precision: {test_precision:.4f} - Recall: {test_recall:.4f} - Specificity: {test_specificity:.4f} - F1: {test_f1:.4f} - Loss: {test_loss:.4f}\\n\")\n",
    "\n",
    "# Visualization Function for Multi-Modal Data (unchanged)\n",
    "def visualize_predictions(images, true_masks, pred_masks, title):\n",
    "    slice_index = images.shape[2] // 2\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        plt.imshow(images[0][:, :, slice_index, i], cmap='gray')\n",
    "        plt.title(f'Modality: {modalities[i]}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(true_masks[0][:, :, slice_index].squeeze(), cmap='jet', alpha=0.7)\n",
    "    plt.title('Ground Truth Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(pred_masks[0][:, :, slice_index].squeeze(), cmap='jet', alpha=0.7)\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Predictions for Test Set\n",
    "X_test_batch, y_test_batch = val_gen.__getitem__(0)\n",
    "y_test_pred_batch = model.predict(X_test_batch)\n",
    "y_test_pred_batch_bin = (y_test_pred_batch > 0.5).astype(np.uint8)\n",
    "\n",
    "visualize_predictions(X_test_batch, y_test_batch, y_test_pred_batch_bin, \"Test Set Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5977a-1ede-49d4-8573-e9bba1b752f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
