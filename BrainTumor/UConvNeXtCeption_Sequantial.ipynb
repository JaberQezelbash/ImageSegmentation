{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819b789-ae5c-4198-86c0-fcbbf07af8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/348 ━━━━━━━━━━━━━━━━━━━━ 23:27:13\n",
      "Accuracy: 0.5543 - Precision: 0.0003 - Recall: 0.0079 - Specificity: 0.5645 - F1: 0.0007 - Loss: 1.7979\n",
      "\n",
      "Batch 2/348 ━━━━━━━━━━━━━━━━━━━━ 00:03:21\n",
      "Accuracy: 0.7764 - Precision: 0.0022 - Recall: 0.0041 - Specificity: 0.7822 - F1: 0.0006 - Loss: 1.3814\n",
      "\n",
      "Batch 3/348 ━━━━━━━━━━━━━━━━━━━━ 00:39:05\n",
      "Accuracy: 0.8097 - Precision: 0.0441 - Recall: 0.3273 - Specificity: 0.8130 - F1: 0.0757 - Loss: 1.5126\n",
      "\n",
      "Batch 4/348 ━━━━━━━━━━━━━━━━━━━━ 01:11:22\n",
      "Accuracy: 0.8544 - Precision: 0.2823 - Recall: 0.3648 - Specificity: 0.8597 - F1: 0.2181 - Loss: 1.2478\n",
      "\n",
      "Batch 5/348 ━━━━━━━━━━━━━━━━━━━━ 01:43:48\n",
      "Accuracy: 0.8682 - Precision: 0.2341 - Recall: 0.4900 - Specificity: 0.8724 - F1: 0.1904 - Loss: 1.2410\n",
      "\n",
      "Batch 6/348 ━━━━━━━━━━━━━━━━━━━━ 02:16:37\n",
      "Accuracy: 0.8887 - Precision: 0.1951 - Recall: 0.4084 - Specificity: 0.8937 - F1: 0.1587 - Loss: 1.2142\n",
      "\n",
      "Batch 7/348 ━━━━━━━━━━━━━━━━━━━━ 02:48:48\n",
      "Accuracy: 0.9040 - Precision: 0.1672 - Recall: 0.3500 - Specificity: 0.9089 - F1: 0.1360 - Loss: 1.1893\n",
      "\n",
      "Batch 8/348 ━━━━━━━━━━━━━━━━━━━━ 03:21:15\n",
      "Accuracy: 0.9152 - Precision: 0.2701 - Recall: 0.3100 - Specificity: 0.9202 - F1: 0.1262 - Loss: 1.1606\n",
      "\n",
      "Batch 9/348 ━━━━━━━━━━━━━━━━━━━━ 03:53:43\n",
      "Accuracy: 0.9245 - Precision: 0.3466 - Recall: 0.3750 - Specificity: 0.9291 - F1: 0.2151 - Loss: 1.0427\n",
      "\n",
      "Batch 10/348 ━━━━━━━━━━━━━━━━━━━━ 04:26:05\n",
      "Accuracy: 0.9318 - Precision: 0.4064 - Recall: 0.4280 - Specificity: 0.9361 - F1: 0.2860 - Loss: 0.9536\n",
      "\n",
      "Batch 11/348 ━━━━━━━━━━━━━━━━━━━━ 04:58:34\n",
      "Accuracy: 0.9374 - Precision: 0.4593 - Recall: 0.4457 - Specificity: 0.9419 - F1: 0.3294 - Loss: 0.8916\n",
      "\n",
      "Batch 12/348 ━━━━━━━━━━━━━━━━━━━━ 05:30:59\n",
      "Accuracy: 0.9400 - Precision: 0.4419 - Recall: 0.4919 - Specificity: 0.9440 - F1: 0.3353 - Loss: 0.8781\n",
      "\n",
      "Batch 13/348 ━━━━━━━━━━━━━━━━━━━━ 06:03:25\n",
      "Accuracy: 0.9443 - Precision: 0.4795 - Recall: 0.5195 - Specificity: 0.9482 - F1: 0.3780 - Loss: 0.8232\n",
      "\n",
      "Batch 14/348 ━━━━━━━━━━━━━━━━━━━━ 06:35:47\n",
      "Accuracy: 0.9480 - Precision: 0.5098 - Recall: 0.5398 - Specificity: 0.9519 - F1: 0.4117 - Loss: 0.7774\n",
      "\n",
      "Batch 15/348 ━━━━━━━━━━━━━━━━━━━━ 07:07:52\n",
      "Accuracy: 0.9503 - Precision: 0.5376 - Recall: 0.5151 - Specificity: 0.9551 - F1: 0.4034 - Loss: 0.7797\n",
      "\n",
      "Batch 16/348 ━━━━━━━━━━━━━━━━━━━━ 07:40:00\n",
      "Accuracy: 0.9525 - Precision: 0.5149 - Recall: 0.5294 - Specificity: 0.9570 - F1: 0.3957 - Loss: 0.7809\n",
      "\n",
      "Batch 17/348 ━━━━━━━━━━━━━━━━━━━━ 08:12:19\n",
      "Accuracy: 0.9551 - Precision: 0.5290 - Recall: 0.5557 - Specificity: 0.9593 - F1: 0.4225 - Loss: 0.7456\n",
      "\n",
      "Batch 18/348 ━━━━━━━━━━━━━━━━━━━━ 08:45:04\n",
      "Accuracy: 0.9573 - Precision: 0.5462 - Recall: 0.5742 - Specificity: 0.9614 - F1: 0.4471 - Loss: 0.7135\n",
      "\n",
      "Batch 19/348 ━━━━━━━━━━━━━━━━━━━━ 09:18:42\n",
      "Accuracy: 0.9590 - Precision: 0.5681 - Recall: 0.5745 - Specificity: 0.9634 - F1: 0.4616 - Loss: 0.6931\n",
      "\n",
      "Batch 20/348 ━━━━━━━━━━━━━━━━━━━━ 09:52:12\n",
      "Accuracy: 0.9605 - Precision: 0.5484 - Recall: 0.5901 - Specificity: 0.9646 - F1: 0.4530 - Loss: 0.6967\n",
      "\n",
      "Batch 21/348 ━━━━━━━━━━━━━━━━━━━━ 10:27:19\n",
      "Accuracy: 0.9621 - Precision: 0.5690 - Recall: 0.5976 - Specificity: 0.9663 - F1: 0.4719 - Loss: 0.6719\n",
      "\n",
      "Batch 22/348 ━━━━━━━━━━━━━━━━━━━━ 11:01:08\n",
      "Accuracy: 0.9636 - Precision: 0.5883 - Recall: 0.6050 - Specificity: 0.9678 - F1: 0.4896 - Loss: 0.6488\n",
      "\n",
      "Batch 23/348 ━━━━━━━━━━━━━━━━━━━━ 11:37:32\n",
      "Accuracy: 0.9651 - Precision: 0.5961 - Recall: 0.6187 - Specificity: 0.9692 - F1: 0.5047 - Loss: 0.6296\n",
      "\n",
      "Batch 24/348 ━━━━━━━━━━━━━━━━━━━━ 12:11:27\n",
      "Accuracy: 0.9663 - Precision: 0.6104 - Recall: 0.6001 - Specificity: 0.9705 - F1: 0.4958 - Loss: 0.6332\n",
      "\n",
      "Batch 25/348 ━━━━━━━━━━━━━━━━━━━━ 12:45:43\n",
      "Accuracy: 0.9675 - Precision: 0.6237 - Recall: 0.6113 - Specificity: 0.9716 - F1: 0.5124 - Loss: 0.6120\n",
      "\n",
      "Batch 26/348 ━━━━━━━━━━━━━━━━━━━━ 13:20:23\n",
      "Accuracy: 0.9686 - Precision: 0.6352 - Recall: 0.6236 - Specificity: 0.9726 - F1: 0.5283 - Loss: 0.5920\n",
      "\n",
      "Batch 27/348 ━━━━━━━━━━━━━━━━━━━━ 14:09:11\n",
      "Accuracy: 0.9693 - Precision: 0.6219 - Recall: 0.6371 - Specificity: 0.9732 - F1: 0.5247 - Loss: 0.5935\n",
      "\n",
      "Batch 28/348 ━━━━━━━━━━━━━━━━━━━━ 14:45:02\n",
      "Accuracy: 0.9703 - Precision: 0.6116 - Recall: 0.6485 - Specificity: 0.9740 - F1: 0.5237 - Loss: 0.5950\n",
      "\n",
      "Batch 29/348 ━━━━━━━━━━━━━━━━━━━━ 15:21:02\n",
      "Accuracy: 0.9711 - Precision: 0.6232 - Recall: 0.6383 - Specificity: 0.9749 - F1: 0.5233 - Loss: 0.5913\n",
      "\n",
      "Batch 30/348 ━━━━━━━━━━━━━━━━━━━━ 15:56:45\n",
      "Accuracy: 0.9716 - Precision: 0.6357 - Recall: 0.6291 - Specificity: 0.9757 - F1: 0.5236 - Loss: 0.5901\n",
      "\n",
      "Batch 31/348 ━━━━━━━━━━━━━━━━━━━━ 16:30:55\n",
      "Accuracy: 0.9724 - Precision: 0.6474 - Recall: 0.6338 - Specificity: 0.9765 - F1: 0.5349 - Loss: 0.5759\n",
      "\n",
      "Batch 32/348 ━━━━━━━━━━━━━━━━━━━━ 17:06:07\n",
      "Accuracy: 0.9731 - Precision: 0.6554 - Recall: 0.6407 - Specificity: 0.9772 - F1: 0.5456 - Loss: 0.5627\n",
      "\n",
      "Batch 33/348 ━━━━━━━━━━━━━━━━━━━━ 17:39:55\n",
      "Accuracy: 0.9737 - Precision: 0.6429 - Recall: 0.6500 - Specificity: 0.9776 - F1: 0.5409 - Loss: 0.5655\n",
      "\n",
      "Batch 34/348 ━━━━━━━━━━━━━━━━━━━━ 18:16:22\n",
      "Accuracy: 0.9744 - Precision: 0.6510 - Recall: 0.6518 - Specificity: 0.9783 - F1: 0.5485 - Loss: 0.5573\n",
      "\n",
      "Batch 35/348 ━━━━━━━━━━━━━━━━━━━━ 18:51:45\n",
      "Accuracy: 0.9749 - Precision: 0.6534 - Recall: 0.6542 - Specificity: 0.9788 - F1: 0.5538 - Loss: 0.5505\n",
      "\n",
      "Batch 36/348 ━━━━━━━━━━━━━━━━━━━━ 19:27:05\n",
      "Accuracy: 0.9756 - Precision: 0.6575 - Recall: 0.6572 - Specificity: 0.9794 - F1: 0.5602 - Loss: 0.5422\n",
      "\n",
      "Batch 37/348 ━━━━━━━━━━━━━━━━━━━━ 20:00:42\n",
      "Accuracy: 0.9762 - Precision: 0.6652 - Recall: 0.6588 - Specificity: 0.9799 - F1: 0.5670 - Loss: 0.5332\n",
      "\n",
      "Batch 38/348 ━━━━━━━━━━━━━━━━━━━━ 20:33:21\n",
      "Accuracy: 0.9768 - Precision: 0.6696 - Recall: 0.6642 - Specificity: 0.9804 - F1: 0.5744 - Loss: 0.5235\n",
      "\n",
      "Batch 39/348 ━━━━━━━━━━━━━━━━━━━━ 21:06:05\n",
      "Accuracy: 0.9772 - Precision: 0.6774 - Recall: 0.6597 - Specificity: 0.9809 - F1: 0.5764 - Loss: 0.5196\n",
      "\n",
      "Batch 40/348 ━━━━━━━━━━━━━━━━━━━━ 21:39:08\n",
      "Accuracy: 0.9777 - Precision: 0.6823 - Recall: 0.6625 - Specificity: 0.9814 - F1: 0.5825 - Loss: 0.5116\n",
      "\n",
      "Batch 41/348 ━━━━━━━━━━━━━━━━━━━━ 22:11:52\n",
      "Accuracy: 0.9781 - Precision: 0.6728 - Recall: 0.6663 - Specificity: 0.9816 - F1: 0.5787 - Loss: 0.5140\n",
      "\n",
      "Batch 42/348 ━━━━━━━━━━━━━━━━━━━━ 22:44:35\n",
      "Accuracy: 0.9786 - Precision: 0.6774 - Recall: 0.6707 - Specificity: 0.9821 - F1: 0.5854 - Loss: 0.5055\n",
      "\n",
      "Batch 43/348 ━━━━━━━━━━━━━━━━━━━━ 23:19:37\n",
      "Accuracy: 0.9790 - Precision: 0.6828 - Recall: 0.6773 - Specificity: 0.9825 - F1: 0.5934 - Loss: 0.4956\n",
      "\n",
      "Batch 44/348 ━━━━━━━━━━━━━━━━━━━━ 23:55:47\n",
      "Accuracy: 0.9794 - Precision: 0.6861 - Recall: 0.6762 - Specificity: 0.9828 - F1: 0.5962 - Loss: 0.4913\n",
      "\n",
      "Batch 45/348 ━━━━━━━━━━━━━━━━━━━━ 00:29:58\n",
      "Accuracy: 0.9797 - Precision: 0.6925 - Recall: 0.6785 - Specificity: 0.9832 - F1: 0.6022 - Loss: 0.4840\n",
      "\n",
      "Batch 46/348 ━━━━━━━━━━━━━━━━━━━━ 01:02:36\n",
      "Accuracy: 0.9801 - Precision: 0.6981 - Recall: 0.6809 - Specificity: 0.9835 - F1: 0.6079 - Loss: 0.4772\n",
      "\n",
      "Batch 47/348 ━━━━━━━━━━━━━━━━━━━━ 01:35:52\n",
      "Accuracy: 0.9804 - Precision: 0.6920 - Recall: 0.6866 - Specificity: 0.9838 - F1: 0.6071 - Loss: 0.4770\n",
      "\n",
      "Batch 48/348 ━━━━━━━━━━━━━━━━━━━━ 02:08:41\n",
      "Accuracy: 0.9807 - Precision: 0.6964 - Recall: 0.6894 - Specificity: 0.9841 - F1: 0.6124 - Loss: 0.4706\n",
      "\n",
      "Batch 49/348 ━━━━━━━━━━━━━━━━━━━━ 02:41:26\n",
      "Accuracy: 0.9810 - Precision: 0.7004 - Recall: 0.6942 - Specificity: 0.9844 - F1: 0.6185 - Loss: 0.4633\n",
      "\n",
      "Batch 50/348 ━━━━━━━━━━━━━━━━━━━━ 03:13:50\n",
      "Accuracy: 0.9814 - Precision: 0.7059 - Recall: 0.6978 - Specificity: 0.9847 - F1: 0.6246 - Loss: 0.4558\n",
      "\n",
      "Batch 51/348 ━━━━━━━━━━━━━━━━━━━━ 03:45:43\n",
      "Accuracy: 0.9817 - Precision: 0.7103 - Recall: 0.7008 - Specificity: 0.9850 - F1: 0.6297 - Loss: 0.4496\n",
      "\n",
      "Batch 52/348 ━━━━━━━━━━━━━━━━━━━━ 04:18:18\n",
      "Accuracy: 0.9819 - Precision: 0.7140 - Recall: 0.7035 - Specificity: 0.9852 - F1: 0.6344 - Loss: 0.4438\n",
      "\n",
      "Batch 53/348 ━━━━━━━━━━━━━━━━━━━━ 04:50:55\n",
      "Accuracy: 0.9822 - Precision: 0.7111 - Recall: 0.7076 - Specificity: 0.9855 - F1: 0.6355 - Loss: 0.4432\n",
      "\n",
      "Batch 54/348 ━━━━━━━━━━━━━━━━━━━━ 05:23:42\n",
      "Accuracy: 0.9825 - Precision: 0.7144 - Recall: 0.7092 - Specificity: 0.9857 - F1: 0.6393 - Loss: 0.4384\n",
      "\n",
      "Batch 55/348 ━━━━━━━━━━━━━━━━━━━━ 05:56:05\n",
      "Accuracy: 0.9828 - Precision: 0.7196 - Recall: 0.6985 - Specificity: 0.9860 - F1: 0.6315 - Loss: 0.4448\n",
      "\n",
      "Batch 56/348 ━━━━━━━━━━━━━━━━━━━━ 06:29:30\n",
      "Accuracy: 0.9829 - Precision: 0.7246 - Recall: 0.6938 - Specificity: 0.9862 - F1: 0.6311 - Loss: 0.4446\n",
      "\n",
      "Batch 57/348 ━━━━━━━━━━━━━━━━━━━━ 07:04:28\n",
      "Accuracy: 0.9831 - Precision: 0.7244 - Recall: 0.6929 - Specificity: 0.9864 - F1: 0.6319 - Loss: 0.4430\n",
      "\n",
      "Batch 58/348 ━━━━━━━━━━━━━━━━━━━━ 07:37:08\n",
      "Accuracy: 0.9833 - Precision: 0.7282 - Recall: 0.6971 - Specificity: 0.9866 - F1: 0.6372 - Loss: 0.4367\n",
      "\n",
      "Batch 59/348 ━━━━━━━━━━━━━━━━━━━━ 08:09:35\n",
      "Accuracy: 0.9836 - Precision: 0.7300 - Recall: 0.6998 - Specificity: 0.9868 - F1: 0.6407 - Loss: 0.4325\n",
      "\n",
      "Batch 60/348 ━━━━━━━━━━━━━━━━━━━━ 08:42:31\n",
      "Accuracy: 0.9838 - Precision: 0.7314 - Recall: 0.7044 - Specificity: 0.9870 - F1: 0.6448 - Loss: 0.4275\n",
      "\n",
      "Batch 61/348 ━━━━━━━━━━━━━━━━━━━━ 09:14:44\n",
      "Accuracy: 0.9837 - Precision: 0.7241 - Recall: 0.7071 - Specificity: 0.9869 - F1: 0.6413 - Loss: 0.4314\n",
      "\n",
      "Batch 62/348 ━━━━━━━━━━━━━━━━━━━━ 09:49:56\n",
      "Accuracy: 0.9839 - Precision: 0.7275 - Recall: 0.7089 - Specificity: 0.9871 - F1: 0.6450 - Loss: 0.4269\n",
      "\n",
      "Batch 63/348 ━━━━━━━━━━━━━━━━━━━━ 10:29:05\n",
      "Accuracy: 0.9841 - Precision: 0.7271 - Recall: 0.7087 - Specificity: 0.9873 - F1: 0.6459 - Loss: 0.4257\n",
      "\n",
      "Batch 64/348 ━━━━━━━━━━━━━━━━━━━━ 11:05:25\n",
      "Accuracy: 0.9842 - Precision: 0.7223 - Recall: 0.7130 - Specificity: 0.9874 - F1: 0.6450 - Loss: 0.4263\n",
      "\n",
      "Batch 65/348 ━━━━━━━━━━━━━━━━━━━━ 11:42:55\n",
      "Accuracy: 0.9844 - Precision: 0.7266 - Recall: 0.7129 - Specificity: 0.9876 - F1: 0.6478 - Loss: 0.4230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.ndimage import zoom, rotate\n",
    "import random\n",
    "\n",
    "# Paths to the dataset\n",
    "image_dir = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\3D\\Task01_BrainTumour\\imagesTr'\n",
    "label_dir = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\3D\\Task01_BrainTumour\\labelsTr'\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1  # Reduced batch size to minimize memory load\n",
    "dim = (128, 128, 128)  # Target dimensions for resizing\n",
    "epochs = 50\n",
    "modalities = [\"FLAIR\", \"T1w\", \"t1gd\", \"T2w\"]\n",
    "\n",
    "# Load filenames and split into training, validation, and testing sets\n",
    "image_files = sorted(os.listdir(image_dir))\n",
    "label_files = sorted(os.listdir(label_dir))\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(image_files, label_files, test_size=0.1, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Generator Class with Data Augmentation\n",
    "class NiftiDataset(Sequence):\n",
    "    def __init__(self, image_files, label_files, image_dir, label_dir, batch_size=2, dim=(128, 128, 128), shuffle=True, augment=False):\n",
    "        self.image_files = image_files\n",
    "        self.label_files = label_files\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.indexes = np.arange(len(self.image_files))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_images = np.zeros((self.batch_size, *self.dim, 4), dtype=np.float32)\n",
    "        batch_masks = np.zeros((self.batch_size, *self.dim, 1), dtype=np.float32)\n",
    "\n",
    "        for i, idx in enumerate(batch_indexes):\n",
    "            img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "            mask_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "\n",
    "            img = nib.load(img_path).get_fdata()\n",
    "            mask = nib.load(mask_path).get_fdata()\n",
    "\n",
    "            img_resized = self.preprocess_image(img)\n",
    "            mask_resized = self.preprocess_mask(mask)\n",
    "\n",
    "            if self.augment:\n",
    "                img_resized, mask_resized = self.augment_data(img_resized, mask_resized)\n",
    "\n",
    "            batch_images[i] = img_resized\n",
    "            batch_masks[i] = mask_resized[..., np.newaxis]\n",
    "\n",
    "        return batch_images, batch_masks\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def preprocess_image(self, img):\n",
    "        img = (img - np.mean(img, axis=(0, 1, 2))) / (np.std(img, axis=(0, 1, 2)) + 1e-7)\n",
    "        zoom_factors = [self.dim[i] / img.shape[i] for i in range(3)] + [1]\n",
    "        img_resized = zoom(img, zoom_factors, order=1)\n",
    "        return img_resized\n",
    "\n",
    "    def preprocess_mask(self, mask):\n",
    "        mask = np.where(mask > 0, 1, 0)\n",
    "        zoom_factors = [self.dim[i] / mask.shape[i] for i in range(3)]\n",
    "        mask_resized = zoom(mask, zoom_factors, order=0)\n",
    "        return mask_resized\n",
    "\n",
    "    def augment_data(self, image, mask):\n",
    "        if random.random() < 0.5:\n",
    "            axis = random.choice([0, 1, 2])\n",
    "            image = np.flip(image, axis=axis)\n",
    "            mask = np.flip(mask, axis=axis)\n",
    "        if random.random() < 0.5:\n",
    "            angle = random.uniform(-10, 10)\n",
    "            axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "            image = rotate(image, angle=angle, axes=axes, reshape=False, order=1)\n",
    "            mask = rotate(mask, angle=angle, axes=axes, reshape=False, order=0)\n",
    "        return image, mask\n",
    "\n",
    "# Define custom loss functions\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-7\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    denominator = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f)\n",
    "    dice_coeff = (2. * intersection + smooth) / (denominator + smooth)\n",
    "    return 1 - dice_coeff\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    bce = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred))\n",
    "    d_loss = dice_loss(y_true, y_pred)\n",
    "    return bce + d_loss\n",
    "\n",
    "# Define custom metrics\n",
    "def custom_precision(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    return tf.clip_by_value(precision, 0, 1)\n",
    "\n",
    "def custom_recall(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    return tf.clip_by_value(recall, 0, 1)\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    precision = custom_precision(y_true, y_pred)\n",
    "    recall = custom_recall(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "    return tf.clip_by_value(f1, 0, 1)\n",
    "\n",
    "def custom_specificity(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "    tn = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    specificity = tn / (tn + fp + 1e-7)\n",
    "    return tf.clip_by_value(specificity, 0, 1)\n",
    "\n",
    "# Define ConvNeXt block with Residual Connections\n",
    "def convnext_block(inputs, filters):\n",
    "    groups = filters if inputs.shape[-1] % filters == 0 else 1\n",
    "    x = tf.keras.layers.Conv3D(filters, kernel_size=7, padding='same', groups=groups)(inputs)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Conv3D(filters, kernel_size=1, padding='same', activation='gelu')(x)\n",
    "    # Residual connection\n",
    "    shortcut = tf.keras.layers.Conv3D(filters, kernel_size=1, padding='same')(inputs)\n",
    "    x = tf.keras.layers.Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "# Define Xception block with Residual Connections\n",
    "def xception_block(inputs, filters):\n",
    "    x = tf.keras.layers.Conv3D(filters, (3, 3, 3), padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv3D(filters, (3, 3, 3), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Residual connection\n",
    "    shortcut = tf.keras.layers.Conv3D(filters, kernel_size=1, padding='same')(inputs)\n",
    "    x = tf.keras.layers.Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "# Define the integrated U-Net model with ConvNeXt, Xception, and Residual Connections\n",
    "def integrated_unet(input_shape=(128, 128, 128, 4)):\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "\n",
    "    # ConvNeXt + Xception Encoder with Residual Connections\n",
    "    c1 = convnext_block(inputs, 32)\n",
    "    c1 = xception_block(c1, 32)\n",
    "    p1 = tf.keras.layers.MaxPooling3D((2, 2, 2))(c1)\n",
    "\n",
    "    c2 = convnext_block(p1, 64)\n",
    "    c2 = xception_block(c2, 64)\n",
    "    p2 = tf.keras.layers.MaxPooling3D((2, 2, 2))(c2)\n",
    "\n",
    "    c3 = convnext_block(p2, 128)\n",
    "    c3 = xception_block(c3, 128)\n",
    "    p3 = tf.keras.layers.MaxPooling3D((2, 2, 2))(c3)\n",
    "\n",
    "    c4 = convnext_block(p3, 256)\n",
    "    c4 = xception_block(c4, 256)\n",
    "\n",
    "    # Decoder with Residual Connections\n",
    "    u5 = tf.keras.layers.UpSampling3D((2, 2, 2))(c4)\n",
    "    u5 = tf.keras.layers.Conv3D(128, (2, 2, 2), activation='relu', padding='same')(u5)\n",
    "    merge5 = tf.keras.layers.concatenate([c3, u5], axis=4)\n",
    "    c5 = convnext_block(merge5, 128)\n",
    "    c5 = xception_block(c5, 128)\n",
    "\n",
    "    u6 = tf.keras.layers.UpSampling3D((2, 2, 2))(c5)\n",
    "    u6 = tf.keras.layers.Conv3D(64, (2, 2, 2), activation='relu', padding='same')(u6)\n",
    "    merge6 = tf.keras.layers.concatenate([c2, u6], axis=4)\n",
    "    c6 = convnext_block(merge6, 64)\n",
    "    c6 = xception_block(c6, 64)\n",
    "\n",
    "    u7 = tf.keras.layers.UpSampling3D((2, 2, 2))(c6)\n",
    "    u7 = tf.keras.layers.Conv3D(32, (2, 2, 2), activation='relu', padding='same')(u7)\n",
    "    merge7 = tf.keras.layers.concatenate([c1, u7], axis=4)\n",
    "    c7 = convnext_block(merge7, 32)\n",
    "    c7 = xception_block(c7, 32)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv3D(1, (1, 1, 1), activation='sigmoid')(c7)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Custom callback to print more metrics at each batch and epoch\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches, val_gen, test_gen):\n",
    "        super().__init__()\n",
    "        self.total_batches = total_batches\n",
    "        self.batch_counter = 1\n",
    "        self.val_gen = val_gen\n",
    "        self.test_gen = test_gen\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.batch_counter = 1  # Reset batch counter at the start of each epoch\n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Batch {self.batch_counter}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Specificity: {specificity:.4f} - F1: {f1:.4f} - Loss: {loss:.4f}\\n\")\n",
    "        self.batch_counter += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_specificity, val_f1 = self.model.evaluate(self.val_gen, verbose=0)\n",
    "        print(f\"Validation Set Performance ━━━━━━━━━━━━━━━━━━━━ {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"Accuracy: {val_accuracy:.4f} - Precision: {val_precision:.4f} - Recall: {val_recall:.4f} - Specificity: {val_specificity:.4f} - F1: {val_f1:.4f} - Loss: {val_loss:.4f}\\n\")\n",
    "        print(f\"End of Epoch {epoch+1}\")\n",
    "\n",
    "# Initialize Generators with Data Augmentation for Training\n",
    "train_gen = NiftiDataset(train_images, train_labels, image_dir, label_dir, batch_size=batch_size, dim=dim, augment=True)\n",
    "val_gen = NiftiDataset(val_images, val_labels, image_dir, label_dir, batch_size=batch_size, dim=dim, shuffle=False)\n",
    "test_gen = NiftiDataset(test_images, test_labels, image_dir, label_dir, batch_size=batch_size, dim=dim, shuffle=False)\n",
    "\n",
    "# Define Model and Compile with Combined Loss Function\n",
    "model = integrated_unet(input_shape=dim + (4,))\n",
    "model.compile(optimizer='adam', loss=combined_loss,\n",
    "              metrics=['accuracy', custom_precision, custom_recall, custom_f1, custom_specificity])\n",
    "\n",
    "# Define Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize Metrics Callback\n",
    "steps_per_epoch = len(train_gen)\n",
    "metrics_callback = MetricsCallback(total_batches=steps_per_epoch, val_gen=val_gen, test_gen=test_gen)\n",
    "\n",
    "# Train Model with Data Augmentation and Modified Loss Function\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=[early_stopping, metrics_callback], verbose=0)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_specificity, test_f1 = model.evaluate(test_gen, verbose=0)\n",
    "\n",
    "# Display Test Set Results\n",
    "current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(f\"Test Set Performance Results ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f} - Precision: {test_precision:.4f} - Recall: {test_recall:.4f} - Specificity: {test_specificity:.4f} - F1: {test_f1:.4f} - Loss: {test_loss:.4f}\\n\")\n",
    "\n",
    "# Visualization Function for Multi-Modal Data (unchanged)\n",
    "def visualize_predictions(images, true_masks, pred_masks, title):\n",
    "    slice_index = images.shape[2] // 2\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        plt.imshow(images[0][:, :, slice_index, i], cmap='gray')\n",
    "        plt.title(f'Modality: {modalities[i]}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(true_masks[0][:, :, slice_index].squeeze(), cmap='jet', alpha=0.7)\n",
    "    plt.title('Ground Truth Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(pred_masks[0][:, :, slice_index].squeeze(), cmap='jet', alpha=0.7)\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Predictions for Test Set\n",
    "X_test_batch, y_test_batch = val_gen.__getitem__(0)\n",
    "y_test_pred_batch = model.predict(X_test_batch)\n",
    "y_test_pred_batch_bin = (y_test_pred_batch > 0.5).astype(np.uint8)\n",
    "\n",
    "visualize_predictions(X_test_batch, y_test_batch, y_test_pred_batch_bin, \"Test Set Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6976da-3409-4f27-9f9f-47356516c44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
