{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487abf7-5b56-4a29-88e9-2228008a1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Passo 1: Leitura e Processamento do JSON \n",
    "## Primeiro, precisamos ler e processar o arquivo JSON para extrair as informações de rótulos e categorias:\n",
    "# Função para carregar o arquivo JSON\n",
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Exemplo de leitura de um JSON\n",
    "json_path = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/train_annotations.coco.json'\n",
    "data = load_json(json_path)\n",
    "\n",
    "# Função para mapear as categorias e imagens\n",
    "def parse_json(data):\n",
    "    categories = {cat['id']: cat['name'] for cat in data['categories']}\n",
    "    images = {img['id']: img['file_name'] for img in data['images']}\n",
    "\n",
    "    return categories, images\n",
    "\n",
    "# Extraindo categorias e imagens\n",
    "categories, images = parse_json(data)\n",
    "\n",
    "print(\"Categorias:\", categories)\n",
    "print(\"Imagens:\", images)\n",
    "\n",
    "\n",
    "categories_df = pd.DataFrame(list(categories.items()), columns=['Category_ID', 'Category_Name'])\n",
    "\n",
    "images_df = pd.DataFrame(list(images.items()), columns=['Image_ID', 'File_Name'])\n",
    "\n",
    "categories_df\n",
    "\n",
    "images_df\n",
    "\n",
    "def load_image_and_mask(image_path, mask_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return image, mask\n",
    "\n",
    "def plot_images_with_masks(images, masks, categories, category_names):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(10, 10))\n",
    "    for i, (image_path, mask_path, category) in enumerate(zip(images, masks, categories)):\n",
    "        image, mask = load_image_and_mask(image_path, mask_path)\n",
    "        row, col = divmod(i, 4)\n",
    "        ax_image = axes[0, col]\n",
    "        ax_image.imshow(image)\n",
    "        ax_image.axis('off')\n",
    "        ax_image.set_title(f\"Categoria: {category_names[category]}\")\n",
    "        ax_mask = axes[1, col]\n",
    "        ax_mask.imshow(image)\n",
    "        ax_mask.imshow(mask, cmap='jet', alpha=0.55)\n",
    "        ax_mask.axis('off')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "category_names = {0: 'vzrad2', 1: 'Caries', 2: 'Crown', 3: 'Filling'}\n",
    "\n",
    "image_dir = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/Dental X_Ray/train'\n",
    "mask_dir = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/Dental X_Ray/train/train_mask'\n",
    "\n",
    "images = [\n",
    "    os.path.join(image_dir, '0a4f2d22-Hematian_Fariba_57y_31052021_132542_jpg.rf.6bfcf8cfb273a9a0767ea25499da98bc.jpg'),\n",
    "    os.path.join(image_dir, '3975890000-jpg_png_jpg.rf.1d1b03af3734eba73e9d12a295578027.jpg'),\n",
    "    os.path.join(image_dir, '4054820000-jpg_png_jpg.rf.1d4a4d0a6fd081280376d10fc085cffa.jpg'),\n",
    "    os.path.join(image_dir, '3794860000-jpg_png_jpg.rf.1cb8e683ddd8a1131d0ffd0c53ee4ac0.jpg')\n",
    "]\n",
    "\n",
    "masks = [\n",
    "    os.path.join(mask_dir, '0a4f2d22-Hematian_Fariba_57y_31052021_132542_jpg.rf.6bfcf8cfb273a9a0767ea25499da98bc.jpg_mask.png'),\n",
    "    os.path.join(mask_dir, '3975890000-jpg_png_jpg.rf.1d1b03af3734eba73e9d12a295578027.jpg_mask.png'),\n",
    "    os.path.join(mask_dir, '4054820000-jpg_png_jpg.rf.1d4a4d0a6fd081280376d10fc085cffa.jpg_mask.png'),\n",
    "    os.path.join(mask_dir, '3794860000-jpg_png_jpg.rf.1cb8e683ddd8a1131d0ffd0c53ee4ac0.jpg_mask.png')\n",
    "]\n",
    "\n",
    "categories = [0, 1, 2, 3]\n",
    "\n",
    "plot_images_with_masks(images, masks, categories, category_names)\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def load_annotations(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    return annotations\n",
    "\n",
    "def load_image_and_mask(image_path, mask_path, target_size=(256, 256)):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    mask = Image.open(mask_path).convert('L')\n",
    "    image = image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    mask = mask.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    image = np.array(image)\n",
    "    mask = np.array(mask)\n",
    "    return image, mask\n",
    "\n",
    "def visualize_batch(images, masks):\n",
    "    batch_size = len(images)\n",
    "    fig, axes = plt.subplots(batch_size, 2, figsize=(10, batch_size * 5))\n",
    "    for i in range(batch_size):\n",
    "        ax_image = axes[i, 0]\n",
    "        ax_image.imshow(images[i])\n",
    "        ax_image.axis('off')\n",
    "        ax_mask = axes[i, 1]\n",
    "        ax_mask.imshow(masks[i], cmap='gray')\n",
    "        ax_mask.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def dataset_generator(image_dir, mask_dir, annotations, batch_size, target_size=(256, 256)):\n",
    "    image_info = annotations['images']\n",
    "    while True:\n",
    "        np.random.shuffle(image_info)\n",
    "        for batch_start in range(0, len(image_info), batch_size):\n",
    "            images = []\n",
    "            masks = []\n",
    "            for i in range(batch_start, min(batch_start + batch_size, len(image_info))):\n",
    "                image_data = image_info[i]\n",
    "                image_filename = image_data['file_name']\n",
    "                image_path = os.path.join(image_dir, image_filename)\n",
    "                mask_filename = f\"{image_filename}_mask.png\"\n",
    "                mask_path = os.path.join(mask_dir, mask_filename)\n",
    "                try:\n",
    "                    image, mask = load_image_and_mask(image_path, mask_path, target_size)\n",
    "                except (FileNotFoundError, ValueError) as e:\n",
    "                    print(f\"Error loading image or mask: {e}\")\n",
    "                    continue\n",
    "                images.append(image / 255.0)\n",
    "                masks.append(mask / 255.0)\n",
    "            yield np.array(images), np.array(masks)\n",
    "\n",
    "def unet_vgg16_model(input_shape):\n",
    "    vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in vgg_base.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = vgg_base.input\n",
    "    c1 = vgg_base.get_layer('block1_conv2').output\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    c2 = vgg_base.get_layer('block2_conv2').output\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    c3 = vgg_base.get_layer('block3_conv3').output\n",
    "    c4 = vgg_base.get_layer('block4_conv3').output\n",
    "\n",
    "    u5 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u5 = concatenate([u5, c3])\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c2])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c1])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "batch_size = 8\n",
    "\n",
    "train_image_dir = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/Dental X_Ray/train'\n",
    "train_mask_dir = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/Dental X_Ray/train/train_mask'\n",
    "train_annotation_file = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/train_annotations.coco.json'\n",
    "\n",
    "valid_image_dir = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/Dental X_Ray/valid'\n",
    "valid_mask_dir = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/Dental X_Ray/valid/valid_mask'\n",
    "valid_annotation_file = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/valid_annotations.coco.json'\n",
    "\n",
    "train_annotations = load_annotations(train_annotation_file)\n",
    "valid_annotations = load_annotations(valid_annotation_file)\n",
    "\n",
    "train_data_gen = dataset_generator(train_image_dir, train_mask_dir, train_annotations, batch_size, target_size=(256, 256))\n",
    "valid_data_gen = dataset_generator(valid_image_dir, valid_mask_dir, valid_annotations, batch_size, target_size=(256, 256))\n",
    "\n",
    "images, masks = next(train_data_gen)\n",
    "visualize_batch(images, masks)\n",
    "\n",
    "train_steps_per_epoch = len(train_annotations['images']) // batch_size\n",
    "valid_steps_per_epoch = len(valid_annotations['images']) // batch_size\n",
    "\n",
    "model = unet_vgg16_model(input_shape)\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_data=valid_data_gen,\n",
    "    validation_steps=valid_steps_per_epoch,\n",
    "    epochs=7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy during Training and Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss during Training and Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training_metrics(history)\n",
    "\n",
    "model.save('vgg16_unet_model.h5')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, classification_report, accuracy_score\n",
    "\n",
    "def evaluate_model(model, data_gen, steps):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        x_batch, y_batch = next(data_gen)\n",
    "        predictions = model.predict(x_batch)\n",
    "        predictions_bin = (predictions >= 0.5).astype(np.int32)\n",
    "        y_true.extend(y_batch.flatten())\n",
    "        y_pred.extend(predictions_bin.flatten())\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            plot_single_prediction(x_batch, y_batch, predictions_bin)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    print(f\"Unique values in y_true: {np.unique(y_true)}\")\n",
    "    print(f\"Unique values in y_pred: {np.unique(y_pred)}\")\n",
    "\n",
    "    if np.array_equal(np.unique(y_true), [0, 1]) and np.array_equal(np.unique(y_pred), [0, 1]):\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "        print(f'F1-Score: {f1}')\n",
    "        print(f'AUC: {auc}')\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, marker='.')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.show()\n",
    "\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    else:\n",
    "        print(\"Error: y_true or y_pred is not in the expected binary format.\")\n",
    "\n",
    "def plot_single_prediction(images, masks_true, masks_pred):\n",
    "    image = images[0]\n",
    "    mask_true = masks_true[0]\n",
    "    mask_pred = masks_pred[0]\n",
    "    mask_true_bin = (mask_true >= 0.5).astype(np.int32)\n",
    "    accuracy = accuracy_score(mask_true_bin.flatten(), mask_pred.flatten())\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(mask_true.squeeze(), cmap='gray')\n",
    "    ax[1].set_title('Original Mask')\n",
    "    ax[1].axis('off')\n",
    "    ax[2].imshow(image)\n",
    "    ax[2].imshow(mask_pred.squeeze(), cmap='jet', alpha=0.5)\n",
    "    ax[2].set_title(f'Predicted Mask (Accuracy: {accuracy:.2f})')\n",
    "    ax[2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_image_dir = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/Dental X_Ray/test'\n",
    "test_mask_dir = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/Dental X_Ray/test/test_mask'\n",
    "test_annotation_file = 'C:/Users/Jaber/OneDrive - University of Florida/Educational/GitHub/Datasets/ImageSegmentation/Dental_XRay_Computacional_Vision_Segmentation/test_annotations.coco.json'\n",
    "test_annotations = load_annotations(test_annotation_file)\n",
    "\n",
    "test_data_gen = dataset_generator(test_image_dir, test_mask_dir, test_annotations, batch_size, target_size=(256, 256))\n",
    "test_steps = len(test_annotations['images']) // batch_size\n",
    "\n",
    "evaluate_model(model, test_data_gen, test_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76173995-084f-402f-86f9-c5e48f75c90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c56d7f-9328-4a35-9834-898185712fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
